{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144ac051-ceb0-4d6c-ad0a-5869b68bad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Collecting pydantic<2\n",
      "  Downloading pydantic-1.10.21-cp312-cp312-win_amd64.whl.metadata (155 kB)\n",
      "     ---------------------------------------- 0.0/155.3 kB ? eta -:--:--\n",
      "     ------- ----------------------------- 30.7/155.3 kB 435.7 kB/s eta 0:00:01\n",
      "     ----------------------- ------------ 102.4/155.3 kB 980.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 155.3/155.3 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from pydantic<2) (4.12.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\orfeo\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading pydantic-1.10.21-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 7.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 6.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.6\n",
      "    Uninstalling pydantic-2.10.6:\n",
      "      Successfully uninstalled pydantic-2.10.6\n",
      "Successfully installed pydantic-1.10.21\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy \"pydantic<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8529f9-87d0-47d5-bd5f-cf2a3aba4506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/12.8 MB 871.5 kB/s eta 0:00:15\n",
      "      --------------------------------------- 0.3/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.6/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.8/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 5.2 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.0/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.3/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.5/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.8/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.4/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.6/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.9/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.2/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 5.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 5.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.7/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.8/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.6/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.9/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.1 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "35c0cd25-1f75-4802-94db-e19fee4358b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 May. Bistritz.—Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse which I got of it from the train and the little I could walk through the streets. I feared to go very far from the station, as we had arrived late and would start as near the correct time as possible. The impression I had was that we were leaving the West and entering the East; the most western of splendid bridges over the Danube, which is here of noble width and depth, took us among the traditions of Turkish rule.\n",
      "\n",
      "We left in pretty good time, and came after nightfall to Klausenburgh. Here I stopped for the night at the Hotel Royale. I had for dinner, or rather supper, a chicken done up some way with red pepper, which was very good but thirsty. (Mem., get recipe for Mina.) I asked the waiter, and he said it was called “paprika hendl,” and that, as it was a national dish, I should be able to get it anywhere along the Carpathians. I found my smattering of German very useful here; indeed, I don’t know how I should be able to get on without it.\n",
      "\n",
      "Having had some time at my disposal when in London, I had visited the British Museum, and made search among the books and maps in the library regarding Transylvania; it had struck me that some foreknowledge of the country could hardly fail to have some importance in dealing with a nobleman of that country. I find that the district he named is in the extreme east of the country, just on the borders of three states, Transylvania, Moldavia and Bukovina, in the midst of the Carpathian mountains; one of the wildest and least known portions of Europe. I was not able to light on any map or work giving the exact locality of the Castle Dracula, as there are no maps of this country as yet to compare with our own Ordnance Survey maps; but I found that Bistritz, the post town named by Count Dracula, is a fairly well-known place. I shall enter here some of my notes, as they may refresh my memory when I talk over my travels with Mina.\n",
      "Longitud del array: 2096\n",
      "\n",
      "3 May. Bistritz.—Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse which I got of it from the train and the little I could walk through the streets. I feared to go very far from the station, as we had arrived late and would start as near the correct time as possible. The impression I had was that we were leaving the West and entering the East; the most western of splendid bridges over the Danube, which is here of noble width and depth, took us among the traditions of Turkish rule.\n",
      "\n",
      "We left in pretty good time, and came after nightfall to Klausenburgh. Here I stopped for the night at the Hotel Royale. I had for dinner, or rather supper, a chicken done up some way with red pepper, which was very good but thirsty. (Mem., get recipe for Mina.) I asked the waiter, and he said it was called “paprika hendl,” and that, as it was a national dish, I should be able to get it anywhere along the Carpathians. I found my smattering of German very useful here; indeed, I don’t know how I should be able to get on without it.\n",
      "\n",
      "Having had some time at my disposal when in London, I had visited the British Museum, and made search among the books and maps in the library regarding Transylvania; it had struck me that some foreknowledge of the country could hardly fail to have some importance in dealing with a nobleman of that country. I find that the district he named is in the extreme east of the country, just on the borders of three states, Transylvania, Moldavia and Bukovina, in the midst of the Carpathian mountains; one of the wildest and least known portions of Europe. I was not able to light on any map or work giving the exact locality of the Castle Dracula, as there are no maps of this country as yet to compare with our own Ordnance Survey maps; but I found that Bistritz, the post town named by Count Dracula, is a fairly well-known place. I shall enter here some of my notes, as they may refresh my memory when I talk over my travels with Mina.\n",
      "Longitud del documento: 453\n"
     ]
    }
   ],
   "source": [
    "import spacy  # Importamos la librería spaCy para procesamiento de lenguaje natural\n",
    "\n",
    "# Cargamos el modelo de lenguaje en inglés \"en_core_web_sm\"\n",
    "# - \"en_core_web_sm\": Modelo pequeño de spaCy para inglés, incluye tokenización, POS tagging, NER, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Abrimos el archivo \"dracula.txt\" en modo lectura (\"r\") con codificación UTF-8\n",
    "# - encoding=\"utf-8\": Asegura que se lean correctamente caracteres especiales.\n",
    "with open(\"dracula.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(text)\n",
    "# Esta longitud representa el número total de caracteres en el archivo, incluidos espacios y saltos de línea.\n",
    "print(\"Longitud del array:\", len(text), end=\"\\n\\n\")\n",
    "\n",
    "# Procesamos el texto con spaCy, lo que divide en tokens, asigna etiquetas, etc.\n",
    "doc = nlp(text)\n",
    "print(doc)\n",
    "\n",
    "# Imprimimos la cantidad de tokens generados en el documento spaCy\n",
    "# La longitud del documento de spaCy es diferente a la del texto original porque:\n",
    "# - En lugar de contar caracteres, spaCy cuenta tokens (palabras, signos de puntuación, números, etc.).\n",
    "# - spaCy puede dividir contracciones en múltiples tokens (por ejemplo, \"don't\" se divide en \"do\" y \"n't\").\n",
    "# - Algunos espacios en blanco y caracteres especiales pueden ser ignorados o tratados de forma distinta.\n",
    "print(\"Longitud del documento:\", len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8a7d88a9-3202-4bd2-974c-1403242a91ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      " \n",
      "M\n",
      "a\n",
      "y\n",
      ".\n",
      " \n",
      "B\n",
      "i\n",
      "s\n",
      "t\n",
      "r\n",
      "i\n",
      "t\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "for token in text[:15]:\n",
    "    print (token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "737bd708-35d7-42bb-bf96-2c6357164c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "May\n",
      ".\n",
      "Bistritz.—Left\n",
      "Munich\n",
      "at\n",
      "8:35\n",
      "P.\n",
      "M.\n",
      ",\n",
      "on\n",
      "1st\n",
      "May\n",
      ",\n",
      "arriving\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:15]:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "991097b6-fd15-4c5c-aac7-95f508e1a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Split 175:\n",
      "called\n",
      "\n",
      "Word Split 176:\n",
      "“paprika\n",
      "\n",
      "Word Split 177:\n",
      "hendl,”\n",
      "\n",
      "Word Split 178:\n",
      "and\n",
      "\n",
      "SpaCy Token 206:\n",
      "“\n",
      "\n",
      "SpaCy Token 207:\n",
      "paprika\n",
      "\n",
      "SpaCy Token 208:\n",
      "hendl\n",
      "\n",
      "SpaCy Token 209:\n",
      ",\n",
      "\n",
      "SpaCy Token 210:\n",
      "”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spaCy es una biblioteca de procesamiento de lenguaje natural (NLP) que realiza un análisis lingüístico avanzado.\n",
    "# Separa las comillas y otros signos de puntuación en tokens individuales.\n",
    "\n",
    "words = text.split()[:200]\n",
    "i=175\n",
    "for token in doc[i:179]:\n",
    "    print (f\"Word Split {i}:\\n{words[i]}\\n\")\n",
    "    i=i+1\n",
    "\n",
    "i=206\n",
    "for token in doc[i:211]:\n",
    "    print (f\"SpaCy Token {i}:\\n{token}\\n\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bba68687-1ef5-4000-a66e-2418ce2f4e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 May.\n",
      "Bistritz.—Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late.\n",
      "Buda-Pesth seems a wonderful place, from the glimpse which I got of it from the train and the little\n",
      "I could walk through the streets.\n",
      "I feared to go very far from the station, as we had arrived late and would start as near the correct time as possible.\n",
      "The impression I had was that we were leaving the West and entering the East; the most western of splendid bridges over the Danube, which is here of noble width and depth, took us among the traditions of Turkish rule.\n",
      "\n",
      "\n",
      "We left in pretty good time, and came after nightfall to Klausenburgh.\n",
      "Here I stopped for the night at the Hotel Royale.\n",
      "I had for dinner, or rather supper, a chicken done up some way with red pepper, which was very good but thirsty.\n",
      "(Mem., get recipe for Mina.)\n",
      "I asked the waiter, and he said it was called “paprika hendl,” and that, as it was a national dish, I should be able to get it anywhere along the Carpathians.\n",
      "I found my smattering of German very useful here; indeed, I don’t know how I should be able to get on without it.\n",
      "\n",
      "\n",
      "Having had some time at my disposal when in London, I had visited the British Museum, and made search among the books and maps in the library regarding Transylvania; it had struck me that some foreknowledge of the country could hardly fail to have some importance in dealing with a nobleman of that country.\n",
      "I find that the district he named is in the extreme east of the country, just on the borders of three states, Transylvania, Moldavia and Bukovina, in the midst of the Carpathian mountains; one of the wildest and least known portions of Europe.\n",
      "I was not able to light on any map or work giving the exact locality of the Castle Dracula, as there are no maps of this country as yet to compare with our own Ordnance Survey maps; but I found that Bistritz, the post town named by Count Dracula, is a fairly well-known place.\n",
      "I shall enter here some of my notes, as they may refresh my memory when I talk over my travels with Mina.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "334907ed-f7bd-4a98-b4c7-67ad6aae143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buda-Pesth seems a wonderful place, from the glimpse which I got of it from the train and the little\n",
      "I could walk through the streets.\n"
     ]
    }
   ],
   "source": [
    "# Obtener la primera oración del documento procesado por spaCy\n",
    "# doc.sents es un generador que contiene todas las oraciones del documento.\n",
    "# Al convertirlo en una lista, podemos acceder a las oraciones por índice.\n",
    "sentence1 = list(doc.sents)[2]\n",
    "sentence2 = list(doc.sents)[3]\n",
    "\n",
    "print(sentence1)\n",
    "print(sentence2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2617af19-2687-4408-a861-1f575c22c6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klausenburgh\n",
      "Tipo de entidad nombrada (código) de sentence1[0]: 384\n",
      "Tipo de entidad nombrada (texto) de sentence1[0]: GPE\n"
     ]
    }
   ],
   "source": [
    "sentence6 = list(doc.sents)[6]\n",
    "print(sentence6[12])\n",
    "# Devuelve un código numérico que representa el tipo de entidad (por ejemplo, 384 para \"PER\").\n",
    "print(\"Tipo de entidad nombrada (código) de sentence1[0]:\", sentence6[12].ent_type)\n",
    "\n",
    "# Devuelve una cadena que describe el tipo de entidad (por ejemplo, \"PER\" para persona).\n",
    "print(\"Tipo de entidad nombrada (texto) de sentence1[0]:\", sentence6[12].ent_type_)\n",
    "\n",
    "# GPE es una etiqueta que representa una entidad geopolítica (Geopolitical Entity). \n",
    "# Esta etiqueta se utiliza para identificar nombres de países, ciudades, estados, regiones y otras entidades geopolíticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccf902-ee12-4eca-ab23-5542599b33ea",
   "metadata": {},
   "source": [
    "## Token Attributes en SpaCy\n",
    "\n",
    "En SpaCy, los **Token Attributes** son propiedades que describen cada token (palabra o signo de puntuación) en un texto procesado. Estos atributos son útiles para realizar análisis lingüísticos más detallados. A continuación se describen algunos de los atributos más comunes, con ejemplos:\n",
    "\n",
    "1. **`text`**: El texto del token.\n",
    "   - Ejemplo: `token.text` devuelve \"Hola\" si el token es \"Hola\".\n",
    "\n",
    "2. **`lemma_`**: La forma base o raíz del token (lematización).\n",
    "   - Ejemplo: `\"corriendo\".lemma_` devuelve \"correr\".\n",
    "\n",
    "3. **`pos_`**: La parte del discurso del token (verbo, sustantivo, etc.).\n",
    "   - Ejemplo: `token.pos_` devuelve \"VERB\" para \"corriendo\".\n",
    "\n",
    "4. **`tag_`**: El etiquetado gramatical detallado (más específico que `pos_`).\n",
    "   - Ejemplo: `\"corriendo\".tag_` podría devolver \"VBG\" (gerundio en inglés).\n",
    "\n",
    "5. **`dep_`**: La relación de dependencia sintáctica con otros tokens.\n",
    "   - Ejemplo: En la frase \"El perro corre\", el token \"corre\" podría tener el valor `\"nsubj\"`, indicando que es el sujeto de la oración.\n",
    "\n",
    "6. **`ent_type_`**: El tipo de entidad si el token es parte de una entidad reconocida (como `PERSON`, `ORG`, `GPE`).\n",
    "   - Ejemplo: `token.ent_type_` devuelve \"PERSON\" si el token es un nombre de persona.\n",
    "\n",
    "7. **`is_stop`**: Indica si el token es una palabra de parada, es decir, una palabra que no tiene mucho valor semántico por sí sola y se usa comúnmente en el lenguaje. Estas palabras suelen ser ignoradas durante el análisis de texto para evitar que el modelo se enfoque en ellas. Palabras de parada incluyen preposiciones, artículos, pronombres, etc.\n",
    "   - Ejemplo: `token.is_stop` es `True` para \"el\", \"de\", \"a\", y `False` para palabras más significativas como \"correr\" o \"animal\".\n",
    "\n",
    "8. **`is_alpha`**: Verifica si el token contiene solo caracteres alfabéticos (sin números ni signos de puntuación).\n",
    "   - Ejemplo: `token.is_alpha` es `True` para \"hola\", pero `False` para \"123\" o \"hola!\".\n",
    "\n",
    "9. **`is_punct`**: Indica si el token es un signo de puntuación.\n",
    "   - Ejemplo: `token.is_punct` es `True` para \"!\" o \",\".\n",
    "\n",
    "10. **`is_digit`**: Verifica si el token es un dígito (número).\n",
    "    - Ejemplo: `token.is_digit` es `True` para \"123\".\n",
    "\n",
    "11. **`shape_`**: Devuelve el patrón de forma del token (letras mayúsculas, minúsculas, números, etc.).\n",
    "    - Ejemplo: `\"Hola\".shape_` devuelve \"Xxxxx\" (una letra mayúscula seguida de letras minúsculas).\n",
    "\n",
    "12. **`lang_`**: Devuelve el código de idioma del texto procesado.\n",
    "    - Ejemplo: Si el texto está en español, `token.lang_` devolverá \"es\".\n",
    "\n",
    "13. **`has_vector`**: Indica si el token tiene un vector asociado (es decir, si SpaCy tiene un vector de palabras preentrenado para esa palabra).\n",
    "    - Ejemplo: `token.has_vector` es `True` si el token tiene un vector preentrenado.\n",
    "\n",
    "14. **`vector`**: Devuelve el vector de palabras (un arreglo de números) asociado al token, si está disponible.\n",
    "    - Ejemplo: `token.vector` devuelve el vector asociado con la palabra \"correr\".\n",
    "\n",
    "15. **`sentiment`**: Devuelve el sentimiento asociado con el token, si está disponible (no siempre se encuentra en todos los modelos).\n",
    "    - Ejemplo: Si el modelo tiene información de sentimientos, `token.sentiment` podría devolver un valor positivo o negativo dependiendo del token.\n",
    "\n",
    "16. **`norm_`**: Devuelve la forma normalizada del token, útil para realizar una normalización adicional del texto, como eliminar acentos o caracteres especiales.\n",
    "    - Ejemplo: `\"Hóla\".norm_` devolvería \"Hola\" si se normaliza la palabra.\n",
    "\n",
    "Estos atributos son herramientas poderosas que te permiten obtener una comprensión más detallada de un texto procesado con SpaCy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "71ef0d72-28e0-4b9a-b236-9b9c6fd434b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.\n",
      "I arrived here yesterday; and my first task is to assure my dear sister of my welfare, and increasing confidence in the success of my undertaking.\n",
      "\n",
      "\n",
      "I am already far north of London; and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves, and fills me with delight.\n",
      "Do you understand this feeling?\n",
      "This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes.\n",
      "Inspirited by this wind of promise, my day dreams become more fervent and vivid.\n",
      "I try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight.\n",
      "There, Margaret, the sun is for ever visible; its broad disk just skirting the horizon, and diffusing a perpetual splendour.\n",
      "There—for with your leave, my sister, I will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe.\n",
      "Its productions and features may be without example, as the phænomena of the heavenly bodies undoubtedly are in those undiscovered solitudes.\n",
      "What may not be expected in a country of eternal light?\n",
      "I may there discover the wondrous power which attracts the needle; and may regulate a thousand celestial observations, that require only this voyage to render their seeming eccentricities consistent for ever.\n",
      "I shall satiate my ardent curiosity with the sight of a part of the world never before visited, and may tread a land never before imprinted by the foot of man.\n",
      "These are my enticements, and they are sufficient to conquer all fear of danger or death, and to induce me to commence this laborious voyage with the joy a child feels when he embarks in a little boat, with his holiday mates, on an expedition of discovery up his native river.\n",
      "But, supposing all these conjectures to be false, you cannot contest the inestimable benefit which I shall confer on all mankind to the last generation, by discovering a passage near the pole to those countries, to reach which at present so many months are requisite; or by ascertaining the secret of the magnet, which, if at all possible, can only be effected by an undertaking such as mine.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"frankenstein.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print (sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a295ee22-52ee-4bf6-ac84-8ee13f702403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.\n"
     ]
    }
   ],
   "source": [
    "sentence = list(doc.sents)[0]\n",
    "print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d5e5b8f7-9012-4d12-80ce-3144918f8d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token principal (head) de token2: rejoice\n",
      "Límite izquierdo de token2: You\n",
      "Límite derecho de token2: .\n",
      "Lema de token2: rejoice\n",
      "Lema del décimo token: accompany\n",
      "Información morfológica del décimo token: Aspect=Perf|Tense=Past|VerbForm=Part\n",
      "Parte de speech (POS) de token2: VERB\n",
      "Dependencia sintáctica de token2: ROOT\n",
      "Idioma de token2: en\n"
     ]
    }
   ],
   "source": [
    "token2 = sentence[2]\n",
    "\n",
    "# El \"head\" es el token del que depende sintácticamente el token actual.\n",
    "print(\"Token principal (head) de token2:\", token2.head)\n",
    "\n",
    "# Es el primer token del subtoken al que pertenece token2.\n",
    "print(\"Límite izquierdo de token2:\", token2.left_edge)\n",
    "\n",
    "# Es el último token del subtoken al que pertenece token2.\n",
    "print(\"Límite derecho de token2:\", token2.right_edge)\n",
    "\n",
    "# Es el lema es la forma canónica de la palabra (por ejemplo, \"corriendo\" -> \"correr\").\n",
    "print(\"Lema de token2:\", token2.lemma_)\n",
    "\n",
    "# Devuelve la forma base de la palabra en la posición 10 de sentence1.\n",
    "print(\"Lema del décimo token:\", sentence[9].lemma_)\n",
    "\n",
    "# Información morfológica\n",
    "# Esto incluye detalles como género, número, tiempo verbal, etc.\n",
    "print(\"Información morfológica del décimo token:\", sentence[9].morph)\n",
    "\n",
    "# Indica la categoría gramatical de la palabra (por ejemplo, \"VERB\", \"NOUN\", \"ADJ\").\n",
    "print(\"Parte de speech (POS) de token2:\", token2.pos_)\n",
    "\n",
    "# Describe la relación sintáctica entre el token y su \"head\" (por ejemplo, \"nsubj\", \"dobj\").\n",
    "print(\"Dependencia sintáctica de token2:\", token2.dep_)\n",
    "\n",
    "# Indica el idioma del texto procesado (por ejemplo, \"es\" para español, \"en\" para inglés).\n",
    "print(\"Idioma de token2:\", token2.lang_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e0b379dc-796e-4fd3-a589-7745efe7573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You PRON nsubj\n",
      "will AUX aux\n",
      "rejoice VERB ROOT\n",
      "to PART aux\n",
      "hear VERB xcomp\n",
      "that SCONJ mark\n",
      "no DET det\n",
      "disaster NOUN nsubj\n",
      "has AUX aux\n",
      "accompanied VERB ccomp\n",
      "the DET det\n",
      "commencement NOUN dobj\n",
      "of ADP prep\n",
      "an DET det\n",
      "enterprise NOUN pobj\n",
      "which PRON dobj\n",
      "you PRON nsubj\n",
      "have AUX aux\n",
      "regarded VERB relcl\n",
      "with ADP prep\n",
      "such ADJ amod\n",
      "evil ADJ amod\n",
      "forebodings NOUN pobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print (token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bf7aa3d3-ce09-4fef-93d9-471853840f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6f43a2f6c2594611add0d55b747f63c4-0\" class=\"displacy\" width=\"4075\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">You</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">rejoice</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">hear</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">no</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">disaster</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">accompanied</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">commencement</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">enterprise</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">which</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">you</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">regarded</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">with</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">such</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">evil</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">forebodings.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,354.0 L748.0,342.0 732.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,89.5 1620.0,89.5 1620.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,177.0 1615.0,177.0 1615.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-8\" stroke-width=\"2px\" d=\"M770,352.0 C770,2.0 1625.0,2.0 1625.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,354.0 L1633.0,342.0 1617.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-10\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,354.0 L1973.0,342.0 1957.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2135.0,354.0 L2143.0,342.0 2127.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-12\" stroke-width=\"2px\" d=\"M2345,352.0 C2345,264.5 2485.0,264.5 2485.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,354.0 L2337,342.0 2353,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-13\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,177.0 2490.0,177.0 2490.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2490.0,354.0 L2498.0,342.0 2482.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-14\" stroke-width=\"2px\" d=\"M2695,352.0 C2695,89.5 3195.0,89.5 3195.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,354.0 L2687,342.0 2703,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-15\" stroke-width=\"2px\" d=\"M2870,352.0 C2870,177.0 3190.0,177.0 3190.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,354.0 L2862,342.0 2878,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-16\" stroke-width=\"2px\" d=\"M3045,352.0 C3045,264.5 3185.0,264.5 3185.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,354.0 L3037,342.0 3053,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-17\" stroke-width=\"2px\" d=\"M2520,352.0 C2520,2.0 3200.0,2.0 3200.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3200.0,354.0 L3208.0,342.0 3192.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-18\" stroke-width=\"2px\" d=\"M3220,352.0 C3220,264.5 3360.0,264.5 3360.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3360.0,354.0 L3368.0,342.0 3352.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-19\" stroke-width=\"2px\" d=\"M3570,352.0 C3570,177.0 3890.0,177.0 3890.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3570,354.0 L3562,342.0 3578,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-20\" stroke-width=\"2px\" d=\"M3745,352.0 C3745,264.5 3885.0,264.5 3885.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3745,354.0 L3737,342.0 3753,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6f43a2f6c2594611add0d55b747f63c4-0-21\" stroke-width=\"2px\" d=\"M3395,352.0 C3395,89.5 3895.0,89.5 3895.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6f43a2f6c2594611add0d55b747f63c4-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3895.0,354.0 L3903.0,342.0 3887.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Renderizar la visualización de las dependencias sintácticas de la oración\n",
    "# - style=\"dep\": Especifica que queremos visualizar las dependencias sintácticas.\n",
    "#   \"dep\" es el estilo para mostrar la estructura de dependencias entre los tokens.\n",
    "displacy.render(sentence, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096ceeb-0108-4732-9370-ed1271e0af43",
   "metadata": {},
   "source": [
    "#### import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"area51.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "28094602-a491-4a07-a8fc-405ba9bc9551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States Air Force ORG\n",
      "the Nevada Test ORG\n",
      "Edwards Air Force Base ORG\n",
      "Homey Airport FAC\n",
      "KXTA ORG\n",
      "FAA ORG\n",
      "XTA ORG\n",
      "Groom Lake PERSON\n",
      "USAF ORG\n",
      "USAF ORG\n",
      "CIA ORG\n",
      "1955 DATE\n",
      "Lockheed ORG\n",
      "Area 51 DATE\n",
      "Top Secret/Sensitive Compartmented Information ORG\n",
      "CIA ORG\n",
      "25 June 2013 DATE\n",
      "FOIA ORG\n",
      "2005 DATE\n",
      "51 CARDINAL\n",
      "Nevada GPE\n",
      "83 miles QUANTITY\n",
      "134 km QUANTITY\n",
      "Las Vegas GPE\n",
      "Rachel PERSON\n",
      "the \"Extraterrestrial Highway ORG\n"
     ]
    }
   ],
   "source": [
    "# Recorremos las entidades encontradas en el documento (doc.ents) y las imprimimos\n",
    "# 'ent.text' es el texto de la entidad y 'ent.label_' es su etiqueta (tipo de entidad, como 'PERSON', 'GPE', etc.)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7b6b73be-a593-4c6e-a4e4-ca3c74603ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Area 51 is the common name of a highly classified \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States Air Force\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (USAF) facility within \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Nevada Test\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and Training Range. A remote detachment administered by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Edwards Air Force Base\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", the facility is officially called \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Homey Airport\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " (ICAO: \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    KXTA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FAA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " LID: \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    XTA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") or \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Groom Lake\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (after the salt flat next to its airfield). Details of its operations are not made public, but the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USAF\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " says that it is an open training range, and it is commonly thought to support the development and testing of experimental aircraft and weapons systems. The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USAF\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CIA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " acquired the site in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1955\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", primarily for flight testing the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lockheed\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " U-2 aircraft.<br><br>The intense secrecy surrounding the base has made it the frequent subject of conspiracy theories and a central component of unidentified flying object (UFO) folklore. It has never been declared a secret base, but all research and occurrences in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Area 51\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " are \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Top Secret/Sensitive Compartmented Information\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (TS/SCI). The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CIA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " publicly acknowledged the base's existence on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    25 June 2013\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", following a Freedom of Information Act (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FOIA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") request filed in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2005\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", and declassified documents detailing its history and purpose.<br><br>Area \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    51\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " is located in the southern portion of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nevada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    83 miles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    134 km\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       ") north-northwest of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Las Vegas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". The surrounding area is a popular tourist destination, including the small town of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rachel\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the &quot;Extraterrestrial Highway\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "&quot;.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizamos las entidades del documento en un formato gráfico usando displacy\n",
    "# 'style=\"ent\"' le indica a displacy que debe renderizar las entidades reconocidas\n",
    "displacy.render(doc, style=\"ent\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4beaa0b6-9e3e-4941-a4b3-89907a052438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c33c413-ad8a-4c29-a877-baf3fd3d9e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.8.0/es_core_news_md-3.8.0-py3-none-any.whl (42.3 MB)\n",
      "     ---------------------------------------- 0.0/42.3 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/42.3 MB 435.7 kB/s eta 0:01:37\n",
      "     ---------------------------------------- 0.1/42.3 MB 1.2 MB/s eta 0:00:37\n",
      "     ---------------------------------------- 0.4/42.3 MB 2.8 MB/s eta 0:00:15\n",
      "      --------------------------------------- 0.5/42.3 MB 3.1 MB/s eta 0:00:14\n",
      "      --------------------------------------- 0.8/42.3 MB 3.5 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 1.3/42.3 MB 4.9 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 1.3/42.3 MB 4.5 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 1.7/42.3 MB 4.7 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 2.0/42.3 MB 5.0 MB/s eta 0:00:09\n",
      "     -- ------------------------------------- 2.4/42.3 MB 5.3 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 2.8/42.3 MB 5.5 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 3.1/42.3 MB 5.7 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 3.5/42.3 MB 5.8 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 3.9/42.3 MB 6.0 MB/s eta 0:00:07\n",
      "     --- ------------------------------------ 4.2/42.3 MB 6.1 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 4.6/42.3 MB 6.2 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 5.0/42.3 MB 6.3 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.4/42.3 MB 6.4 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 5.7/42.3 MB 6.5 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 6.1/42.3 MB 6.6 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 6.5/42.3 MB 6.7 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 6.9/42.3 MB 6.8 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 7.3/42.3 MB 6.8 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 7.6/42.3 MB 6.9 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 8.0/42.3 MB 6.9 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 8.4/42.3 MB 7.0 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 8.8/42.3 MB 7.1 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 9.2/42.3 MB 7.1 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 9.6/42.3 MB 7.2 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 10.0/42.3 MB 7.3 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 10.4/42.3 MB 7.7 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 10.8/42.3 MB 8.0 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 11.2/42.3 MB 8.2 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 11.6/42.3 MB 8.1 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 12.0/42.3 MB 8.3 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 12.4/42.3 MB 8.3 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 12.7/42.3 MB 8.3 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 13.2/42.3 MB 8.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 13.5/42.3 MB 8.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 14.0/42.3 MB 8.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 14.4/42.3 MB 8.4 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 14.8/42.3 MB 8.5 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 15.2/42.3 MB 8.5 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 15.6/42.3 MB 8.5 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 16.0/42.3 MB 8.5 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 16.4/42.3 MB 8.6 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 16.8/42.3 MB 8.6 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 17.3/42.3 MB 8.7 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 17.6/42.3 MB 8.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 18.1/42.3 MB 8.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 18.5/42.3 MB 8.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 18.9/42.3 MB 8.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 19.3/42.3 MB 8.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 19.8/42.3 MB 8.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 20.2/42.3 MB 8.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 20.6/42.3 MB 8.8 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 21.0/42.3 MB 9.0 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 21.5/42.3 MB 9.0 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 21.9/42.3 MB 9.0 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.4/42.3 MB 9.0 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 22.8/42.3 MB 9.1 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 23.2/42.3 MB 9.1 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 23.7/42.3 MB 9.1 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 24.1/42.3 MB 9.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 24.5/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 25.0/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 25.4/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 25.9/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 26.3/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 26.8/42.3 MB 9.5 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 27.2/42.3 MB 9.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 27.7/42.3 MB 9.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 28.2/42.3 MB 9.5 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 28.6/42.3 MB 9.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 29.1/42.3 MB 9.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 29.5/42.3 MB 9.6 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 29.9/42.3 MB 9.8 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 30.4/42.3 MB 9.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 30.7/42.3 MB 9.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 31.0/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 31.4/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 31.7/42.3 MB 9.4 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 32.1/42.3 MB 9.2 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 32.4/42.3 MB 9.2 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 32.8/42.3 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 33.1/42.3 MB 9.0 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 33.5/42.3 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 33.8/42.3 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 34.2/42.3 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 34.5/42.3 MB 8.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 34.9/42.3 MB 8.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 35.3/42.3 MB 8.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 35.7/42.3 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 36.0/42.3 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 36.4/42.3 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 36.8/42.3 MB 8.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.2/42.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.6/42.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 37.9/42.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.3/42.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 38.7/42.3 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.1/42.3 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.6/42.3 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.9/42.3 MB 8.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.3/42.3 MB 8.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.7/42.3 MB 8.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 41.1/42.3 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.6/42.3 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.0/42.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.3/42.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 42.3/42.3 MB 8.1 MB/s eta 0:00:00\n",
      "Installing collected packages: es-core-news-md\n",
      "Successfully installed es-core-news-md-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fb261b8-24b0-4f76-9ade-5f082fcf3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "045de18a-9c11-49ee-81ca-73943b70483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/33.5 MB 330.3 kB/s eta 0:01:42\n",
      "     ---------------------------------------- 0.1/33.5 MB 1.1 MB/s eta 0:00:32\n",
      "     ---------------------------------------- 0.3/33.5 MB 2.4 MB/s eta 0:00:14\n",
      "      --------------------------------------- 0.5/33.5 MB 3.1 MB/s eta 0:00:11\n",
      "      --------------------------------------- 0.8/33.5 MB 3.9 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 1.5/33.5 MB 5.6 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 1.9/33.5 MB 5.9 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 2.2/33.5 MB 6.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 2.6/33.5 MB 6.3 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 2.9/33.5 MB 6.4 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 3.3/33.5 MB 6.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 3.6/33.5 MB 6.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 4.0/33.5 MB 6.8 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 4.4/33.5 MB 6.9 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 4.8/33.5 MB 6.9 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 5.1/33.5 MB 7.0 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 5.5/33.5 MB 7.0 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 5.9/33.5 MB 7.1 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 6.3/33.5 MB 7.2 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 6.6/33.5 MB 7.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 7.0/33.5 MB 7.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 7.4/33.5 MB 7.2 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 7.7/33.5 MB 7.3 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 8.1/33.5 MB 7.3 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 8.5/33.5 MB 7.3 MB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 8.9/33.5 MB 7.4 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 9.2/33.5 MB 7.5 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 9.7/33.5 MB 7.5 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 10.1/33.5 MB 7.5 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 10.5/33.5 MB 8.1 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 10.8/33.5 MB 8.2 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 11.3/33.5 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 11.6/33.5 MB 8.1 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 12.1/33.5 MB 8.1 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 12.4/33.5 MB 8.1 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 12.8/33.5 MB 8.3 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 13.2/33.5 MB 8.3 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 13.6/33.5 MB 8.3 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 14.0/33.5 MB 8.3 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 14.4/33.5 MB 8.3 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 14.9/33.5 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 15.2/33.5 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 15.6/33.5 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 16.0/33.5 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 16.4/33.5 MB 8.5 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 16.8/33.5 MB 8.5 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 17.2/33.5 MB 8.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 17.6/33.5 MB 8.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 18.1/33.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 18.5/33.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 18.9/33.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 19.3/33.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 19.8/33.5 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 20.2/33.5 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 20.6/33.5 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 21.0/33.5 MB 8.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 21.5/33.5 MB 8.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 21.9/33.5 MB 8.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 22.3/33.5 MB 9.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 22.8/33.5 MB 9.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 23.2/33.5 MB 9.1 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 23.6/33.5 MB 9.1 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 24.1/33.5 MB 9.1 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 24.5/33.5 MB 9.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 25.0/33.5 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 25.4/33.5 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 25.9/33.5 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 26.3/33.5 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 26.8/33.5 MB 9.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 27.2/33.5 MB 9.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 27.7/33.5 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 28.1/33.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 28.5/33.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 29.0/33.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 29.5/33.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 29.9/33.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 30.4/33.5 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 30.9/33.5 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 31.3/33.5 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 31.8/33.5 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 32.2/33.5 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  32.7/33.5 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  33.2/33.5 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  33.5/33.5 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.5/33.5 MB 9.6 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eae3f5-4dc4-4a9e-b632-893f62901b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería PyDictionary, que permite obtener sinónimos, antónimos y significados de palabras.\n",
    "from PyDictionary import PyDictionary  \n",
    "\n",
    "# Creamos una instancia de PyDictionary para utilizar sus funciones.\n",
    "dictionary = PyDictionary()  \n",
    "\n",
    "# Definimos una cadena de texto de ejemplo.\n",
    "text = \"Tom loves to eat chocolate\"  \n",
    "\n",
    "# Dividimos el texto en palabras individuales usando el método split(),\n",
    "# que separa la cadena en una lista de palabras basándose en los espacios.\n",
    "words = text.split()  \n",
    "\n",
    "# Iteramos sobre cada palabra en la lista 'words'\n",
    "for word in words:  \n",
    "    # Obtenemos los sinónimos de la palabra actual usando el método synonym().\n",
    "    # Este método devuelve una lista de sinónimos para la palabra dada.\n",
    "    syns = dictionary.synonym(word)  \n",
    "    \n",
    "    # Imprimimos la palabra junto con hasta 5 de sus sinónimos.\n",
    "    # La sintaxis syns[0:5] toma los primeros 5 sinónimos de la lista (si existen).\n",
    "    print(f\"{word}: {syns[0:5]}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ddfef3-c632-4475-86de-e9b4f76ba0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043bb0b1-4cf0-4253-a318-10d5067c87ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Orfeo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinónimos de 'happy': {'felicitous', 'well-chosen', 'happy', 'glad'}\n",
      "Definiciones de 'happy': ['enjoying or showing or marked by joy or pleasure', 'marked by good fortune', 'eagerly disposed to act or to be of service', 'well expressed and to the point']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Descargar los datos de WordNet (solo la primera vez)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Obtener sinónimos de una palabra\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Obtener definiciones de una palabra\n",
    "def get_definitions(word):\n",
    "    definitions = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        definitions.append(syn.definition())\n",
    "    return definitions\n",
    "\n",
    "# Ejemplo de uso\n",
    "word = \"happy\"\n",
    "print(f\"Sinónimos de '{word}': {get_synonyms(word)}\")\n",
    "print(f\"Definiciones de '{word}': {get_definitions(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "302bf5d6-9328-433f-b9d0-3e349d2763b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I arrived here yesterday; and my first task is to assure my dear sister of my welfare, and increasing confidence in the success of my undertaking.\n",
      "\n",
      "\n",
      "[-7.01530576e-01  2.39376962e-01 -1.89651623e-01 -4.08036299e-02\n",
      " -1.81695238e-01  4.12037931e-02  3.35094370e-02 -1.66771203e-01\n",
      "  3.57172750e-02  2.06801605e+00 -2.28182629e-01 -8.50432552e-03\n",
      " -5.48411980e-02  1.20739549e-01 -1.67488724e-01  3.32906917e-02\n",
      " -1.24966264e-01  8.48675609e-01 -1.67296201e-01  3.50403078e-02\n",
      " -5.51085211e-02 -1.49846924e-02  7.43212402e-02 -2.56478399e-01\n",
      " -6.28851354e-02 -8.91163349e-02 -5.98636409e-03 -8.02313536e-02\n",
      " -8.47796053e-02 -1.73405156e-01 -1.21948957e-01  7.82050118e-02\n",
      " -1.55026438e-02 -3.68612148e-02  7.88801834e-02  1.80211157e-01\n",
      " -9.75729711e-03  4.07703631e-02 -2.46105082e-02 -4.39540250e-03\n",
      "  8.23048204e-02 -3.61623317e-02 -2.55633146e-03 -1.51528362e-02\n",
      "  7.84843266e-02 -2.06504077e-01 -2.92854682e-02  1.51712745e-01\n",
      " -3.43521447e-06  5.33212833e-02 -1.21856295e-01  1.68842569e-01\n",
      "  1.13984101e-01 -1.28053343e-02  7.36300424e-02  8.87940824e-03\n",
      " -5.69123309e-03 -7.36303180e-02  3.17648686e-02 -7.72592202e-02\n",
      " -1.25062332e-01 -1.69528380e-01 -7.77264014e-02  9.25661325e-02\n",
      "  2.13812292e-01 -3.97396013e-02  1.09655522e-02 -4.58767414e-02\n",
      "  8.75614360e-02  2.15758942e-02  1.43666655e-01  1.11642689e-01\n",
      "  1.49542972e-01 -2.03046631e-02  3.53302918e-02  1.85742393e-01\n",
      "  7.74241909e-02 -1.01896620e-03  8.55930522e-03  1.59873948e-01\n",
      " -1.30334631e-01  1.33877337e-01 -1.74651388e-02 -9.79428515e-02\n",
      " -4.89518061e-05 -1.17606558e-01  2.74594367e-01 -2.68743217e-01\n",
      "  4.62816935e-03 -3.54047865e-02 -1.17618673e-01 -3.47111896e-02\n",
      " -1.19048961e-01  5.15533648e-02 -4.52658683e-02 -2.25975662e-01\n",
      "  3.98923010e-02 -3.46870832e-02  1.54971436e-01 -1.48921395e-02\n",
      " -1.09135427e-01  2.90719010e-02 -1.41518921e-01  3.40653285e-02\n",
      "  2.55056053e-01 -1.23588669e+00  8.91542211e-02 -3.86696844e-03\n",
      " -1.03426382e-01 -1.17750645e-01 -3.80964056e-02 -1.53604582e-01\n",
      " -3.13016661e-02 -2.04745322e-01 -7.27423057e-02 -5.71586341e-02\n",
      " -2.75778696e-02 -2.25529343e-01  3.33903655e-02 -2.31387526e-01\n",
      "  1.67319462e-01  4.55305316e-02 -2.30970029e-02 -1.83850378e-01\n",
      " -6.22553676e-02 -5.38371466e-02 -7.75336549e-02 -1.29576474e-01\n",
      " -1.17594853e-01 -9.33621302e-02  8.26720968e-02 -2.58934330e-02\n",
      "  1.99693013e-02  1.61890268e-01  1.45866409e-01 -6.59570619e-02\n",
      " -1.35392696e-01  1.35123674e-02 -4.53191884e-02 -1.24488827e-02\n",
      " -1.22061098e+00  2.12175518e-01  1.96849331e-01  5.99280233e-03\n",
      " -8.83474052e-02 -8.31453577e-02 -4.92257439e-02  1.16417170e-01\n",
      "  8.23502894e-03 -4.24107350e-02 -4.64656763e-03  1.85465500e-01\n",
      "  4.12991196e-02 -5.35852872e-02  1.02115043e-01 -2.23485995e-02\n",
      " -4.48491722e-02  9.39314589e-02  2.03345716e-03 -3.55313182e-01\n",
      " -6.12007305e-02 -2.81083267e-02  6.23367429e-02 -6.39074296e-02\n",
      " -1.03672013e-01 -3.97220962e-02  1.47889286e-01 -3.67162377e-02\n",
      " -3.66820283e-02 -1.15210220e-01 -1.08694896e-01 -1.87711582e-01\n",
      "  1.01290323e-01 -9.27646533e-02  1.82937067e-02 -1.42522352e-02\n",
      " -7.44987205e-02  3.59985568e-02  6.19442686e-02 -6.73614591e-02\n",
      " -1.08714588e-01 -8.44252110e-02 -3.51308703e-01  1.46096364e-01\n",
      " -1.22690201e-01  1.27215371e-01 -7.65691921e-02  2.26992331e-02\n",
      "  3.76589485e-02  1.12299643e-01  2.21263826e-01 -7.21335188e-02\n",
      " -2.71379296e-02  5.17884642e-02 -7.13756233e-02 -1.01625592e-01\n",
      "  7.09152445e-02 -1.74280092e-01 -1.34469137e-01  1.52753100e-01\n",
      "  1.25294074e-01 -1.11465856e-01 -7.58438483e-02  4.34940383e-02\n",
      " -1.76354930e-01 -3.13817011e-03 -1.33231683e-02  5.38821034e-02\n",
      "  4.35075983e-02  1.56081632e-01  9.98412725e-03  3.22906300e-02\n",
      " -2.49388814e-03 -7.50783980e-02  7.47329295e-02  1.31410025e-02\n",
      "  2.29529664e-02  1.27222329e-01 -1.62962019e-01 -5.83160073e-02\n",
      " -8.61824378e-02  3.29867341e-02 -1.80541828e-01 -1.00216322e-01\n",
      "  3.21678706e-02 -1.68420956e-01 -6.43621609e-02 -6.26626387e-02\n",
      " -5.26232012e-02 -6.72872216e-02  7.19669685e-02 -4.63964455e-02\n",
      "  1.45512477e-01  3.90383340e-02  5.28880022e-03  8.18895251e-02\n",
      "  1.17530987e-01 -9.56811458e-02 -1.17276743e-01 -6.89957961e-02\n",
      " -3.41895707e-02 -3.38602928e-03 -3.71055640e-02  1.21136777e-01\n",
      "  1.00526080e-01 -1.45028442e-01  9.89183262e-02 -2.62706101e-01\n",
      " -7.41316080e-02  1.88731253e-01  4.43482958e-02 -4.15707491e-02\n",
      "  5.60485013e-02 -8.27899799e-02 -6.61281496e-02  2.59843171e-01\n",
      " -1.79977659e-02 -6.21478148e-02  7.55950212e-02  9.35344622e-02\n",
      " -1.26938790e-01  7.31876567e-02  1.42147206e-02  1.28918931e-01\n",
      "  1.16820261e-01 -1.59964308e-01  4.31019766e-03  8.76506120e-02\n",
      "  4.76864457e-01  6.13970011e-02  1.83269829e-01 -8.21696892e-02\n",
      " -1.10228159e-01 -2.65602261e-01 -1.47214726e-01 -2.98359022e-02\n",
      " -2.26323796e-03 -3.74749042e-02 -7.59830028e-02  3.15221965e-01\n",
      " -8.21438283e-02 -8.45936418e-04 -4.74058231e-03  1.44513458e-01\n",
      " -2.81750038e-03  8.58439580e-02  4.87962887e-02 -1.89721197e-01\n",
      "  7.06025586e-02  4.00225148e-02 -1.63048327e-01  1.23748370e-01\n",
      " -3.27451937e-02 -4.38766070e-02  3.66106369e-02  1.41239315e-01\n",
      "  2.09165868e-04 -8.27790871e-02  2.43675690e-02 -3.13796885e-02]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "with open(\"frankenstein.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "sentence = list(doc.sents)[1]\n",
    "print (sentence)\n",
    "\n",
    "# Devuelve el vector de embeddings del primer token de la oración.\n",
    "# Estos vectores son útiles para representar palabras de manera numérica y realizar tareas de NLP, como calcular similitudes semánticas o entrenar modelos de aprendizaje automático.\n",
    "print(sentence.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67287695-5e08-4b97-90b1-273ec2dcfe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHINCHILLA', 'pooch', 'CORGI', 'cattery', 'ADOPTED', 'adopt', 'cockapoos', 'CHINCHILLAS', 'goldendoodles', 'sighthound']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos una palabra de referencia para la búsqueda de palabras similares\n",
    "your_word = \"dog\"  \n",
    "\n",
    "# Buscamos los vectores más similares a la palabra \"dog\" en el vocabulario de spaCy.\n",
    "# Se obtiene el vector de la palabra \"dog\" y se busca en la base de datos de vectores de spaCy.\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10\n",
    ")\n",
    "\n",
    "# Extraemos las 10 palabras más similares (según la distancia en el espacio vectorial).\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]  \n",
    "\n",
    "# Obtenemos las distancias entre \"dog\" y las palabras encontradas.\n",
    "distances = ms[2]  \n",
    "\n",
    "# Imprimimos la lista de palabras más similares.\n",
    "print(words)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75b8cd7d-9bb4-4f17-8b28-855d5f44ef2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: 'El'\n",
      "Vector: [-6.1196e+00  9.9648e+00 -3.9819e+00  8.9273e+00 -7.3323e+00  2.3622e+00\n",
      "  6.0540e+00  7.0561e+00 -3.3268e+00 -1.6534e+01  3.2897e+00 -7.4372e+00\n",
      " -4.0304e+00 -1.0274e+01 -3.6511e+00  1.9179e+00  5.1057e+00  5.9131e+00\n",
      " -6.3222e+00 -2.5703e+00 -5.7388e+00 -5.2501e+00 -7.9416e-01  8.2890e+00\n",
      "  3.6103e+00 -1.2552e+00  1.2473e+01  5.6777e+00 -4.4452e+00 -1.4430e+00\n",
      " -2.0346e+00 -7.6281e+00 -3.7715e+00  4.4597e+00  9.0411e-01 -1.0366e+01\n",
      " -3.8643e+00  6.4961e+00  8.0252e-01 -2.5458e+00 -5.5798e+00 -3.2279e+00\n",
      " -4.8855e+00 -2.2025e-01  2.6544e+00  3.5740e+00  1.0647e+01  1.8326e-01\n",
      " -1.0182e+01  3.5352e+00 -1.8949e+00  1.6235e+00  1.2020e+00 -2.8645e+00\n",
      "  1.6326e+01  4.2531e+00 -7.9472e+00 -3.0151e+00 -5.9569e+00  1.6463e+00\n",
      " -3.4114e+00  1.7201e+00 -2.5783e+00 -1.7547e+00  7.2678e-01 -5.6578e+00\n",
      "  5.7816e-01  1.6420e+00 -8.6577e+00 -1.0265e+01 -1.2422e+01 -3.0335e+00\n",
      " -6.4310e-03  7.4140e-02  1.8943e+00  1.8212e+00 -1.9917e+00 -9.8692e-01\n",
      " -2.4345e+00 -2.7045e+00 -1.6569e+00  3.2044e+00 -4.2677e+00  3.7444e+00\n",
      "  5.3684e+00 -4.6888e+00 -3.4801e+00 -2.3751e+00  2.4219e-01  6.5618e+00\n",
      "  1.0581e+00  3.0899e-02 -3.8852e+00 -7.9132e+00  4.2613e+00 -3.9681e+00\n",
      " -6.3609e+00 -4.3228e+00 -2.5284e+00 -1.1820e+01  4.5031e-01 -9.2320e-01\n",
      "  7.2991e-01  5.0082e+00  2.9680e+00 -4.6662e+00  3.9011e+00 -8.3508e+00\n",
      "  1.8743e-01  4.2837e-01  6.8329e+00  1.9465e+00  1.1185e+00  1.6637e+00\n",
      " -1.2592e+01 -4.5613e+00  4.4742e+00 -1.8496e+00  9.3852e+00 -1.1091e-01\n",
      "  5.1581e+00  1.0607e+00  3.9731e+00 -6.7869e+00 -8.5532e-01  3.5596e+00\n",
      "  4.7766e+00  3.3406e+00 -1.5362e+00 -6.0091e+00  8.5367e+00 -6.7445e+00\n",
      " -1.0320e+01 -5.7055e+00  5.6333e+00  8.1384e-01  7.3647e-01 -8.9362e+00\n",
      "  5.3404e+00  8.6627e+00  3.0773e+00 -2.1884e-01 -3.6810e+00  2.6975e+00\n",
      " -3.1328e+00 -5.5718e+00 -9.5403e+00 -8.5346e+00 -1.1443e+01 -8.3358e-01\n",
      " -3.5227e+00  1.8621e+00  7.0084e+00  2.1869e+00  5.2332e+00  3.4095e+00\n",
      "  8.7993e+00  1.0388e+01 -2.2252e+00 -5.8132e+00 -1.3401e+00 -4.1399e-02\n",
      "  3.7079e+00 -4.2477e-01 -5.0658e+00 -5.7132e+00 -2.0442e+00  1.2617e+01\n",
      "  7.7531e-01 -4.9931e+00 -9.7148e+00  1.2535e+00 -5.5991e+00  2.5196e+00\n",
      " -7.3744e-01  4.8653e+00 -5.0211e+00 -1.0607e+01 -4.1380e+00 -6.2852e-01\n",
      "  5.8603e+00 -9.9830e+00 -3.6543e+00 -2.3147e+00  5.7743e+00 -7.9183e+00\n",
      " -4.4049e-01  3.7893e+00 -3.5195e+00  1.0181e+00 -8.2755e+00 -2.1025e+01\n",
      " -2.6297e+00 -1.5653e+00  1.2179e+01  4.8241e+00 -7.1382e+00  3.6191e+00\n",
      "  1.4859e+00 -2.6803e+00 -4.0816e+00 -4.1399e+00  7.9903e+00  8.1697e-01\n",
      " -7.1985e+00 -3.6465e+00  4.2598e+00  5.7240e+00  4.5455e+00  2.3530e+00\n",
      " -2.6931e+00 -5.0274e+00  4.6522e+00 -1.3149e+00  4.5925e-01  5.4534e+00\n",
      " -2.9381e+00  1.6317e+00 -8.5570e+00 -6.8936e+00  4.7481e+00  6.9711e-01\n",
      " -4.1618e+00 -9.5567e+00  6.4959e+00  4.6644e+00 -8.6836e+00  6.7605e+00\n",
      " -9.1663e+00  7.4795e+00  1.2347e+00  1.1915e+01  3.1394e+00 -9.9515e-01\n",
      "  8.8056e+00 -2.3230e+00 -1.8383e-01  4.9865e+00  1.8372e+00  3.7818e+00\n",
      " -2.4489e+00 -7.5424e+00 -1.0373e+01 -4.7892e+00 -4.3213e+00  1.8461e+00\n",
      " -5.5918e+00  1.2312e+00  8.9512e-01 -2.4554e+00  5.4668e+00 -3.6330e+00\n",
      "  2.3527e+00  9.7251e+00  6.4519e+00 -1.1509e+01  4.1637e+00  2.4669e+00\n",
      " -7.7304e+00  5.4051e+00  2.1995e+00  2.3987e+00 -4.0442e+00  1.1259e+00\n",
      " -4.1873e+00 -9.7096e-01 -1.2313e+01 -1.3904e+01  4.6186e+00  6.2412e+00\n",
      " -1.0234e+01 -1.2701e+01  1.2601e+01  6.8942e+00  1.0824e+01 -1.1650e+01\n",
      " -8.7880e-01  4.9003e+00  6.2578e+00 -4.4339e+00 -7.2792e-01 -5.7234e+00\n",
      "  3.7576e+00 -5.7279e+00  9.3258e+00  5.7818e+00 -1.3857e+01 -1.0957e+01\n",
      " -4.4872e+00  3.5826e-01 -4.8129e+00 -9.2207e+00 -3.0412e+00 -8.7034e+00\n",
      " -4.3583e+00  3.2783e+00  4.0971e+00 -5.0808e+00  3.4057e+00  1.5592e+00]\n",
      "------------------------------\n",
      "Token 1: 'gato'\n",
      "Vector: [-0.3633   -0.48841  -2.7278    2.3517    1.6459   -0.14743  -3.071\n",
      "  0.43446   2.376     2.939    -3.1857   -1.3094    1.8059   -1.7071\n",
      " -0.98022  -1.7231   -2.4354   -2.9543    0.64656  -2.2967    0.34807\n",
      " -2.8105    1.1762    3.2424    0.18839  -2.8763   -0.85192  -1.8524\n",
      " -2.2221    3.7512   -3.0247    0.58154   1.5971    1.6494   -0.64399\n",
      "  1.8924    0.77904  -1.6907   -0.94121  -0.28061   0.9683    2.1969\n",
      "  1.0211    2.092    -2.2401   -1.7143   -0.44994   0.071469  1.3566\n",
      " -3.014     1.2851    0.27378   1.2742    0.55748  -3.4488   -0.16216\n",
      " -0.24527  -0.93641   0.94152   2.3763    0.53665   0.89439   2.1799\n",
      "  1.0793    1.0502   -2.5831    1.8174   -2.0587   -3.7014    3.5125\n",
      " -2.5576   -0.21629  -3.1582   -2.1678    2.6088   -2.1502   -1.7187\n",
      "  2.1234   -3.6737   -4.2948    0.28615  -1.4556    1.5618   -1.1729\n",
      " -4.9728    0.60744  -2.2477   -3.1606   -1.5781   -0.69417  -0.32749\n",
      "  1.5317   -1.2592   -2.7738    1.14      0.46923  -0.87273  -0.21848\n",
      "  0.22565  -0.59636  -1.9601    1.9623    1.1068    3.9961   -0.011909\n",
      "  2.8441   -3.4065   -1.0487   -0.22615  -3.1205    0.43137   1.8768\n",
      " -0.61303   1.5307   -1.6693    2.6521   -3.9042    0.65099  -3.865\n",
      "  0.071395 -0.19876  -0.53764  -0.7734   -0.85461   1.8388   -2.048\n",
      "  1.4305    2.0518    1.926    -0.17568  -0.87003   2.5502    4.247\n",
      " -1.3054    1.5488    0.43527  -2.2223    2.726    -0.1085    0.89474\n",
      " -0.42681  -1.181    -0.83361  -1.0829    0.98432   5.2924   -2.1318\n",
      " -0.1971   -0.73554   4.4138   -1.0698    1.0611    0.42646   2.5666\n",
      "  0.13698   0.3878    3.4338    0.80204   1.9867   -6.2636    1.2783\n",
      "  2.1879    1.9532    4.5902   -0.40992   4.4139    1.3003   -3.671\n",
      " -1.1165    3.484     0.93629   1.5106   -2.1363   -0.057473 -0.73711\n",
      "  0.090913  2.6206    3.3185   -1.4552    2.1621    0.47805   0.78012\n",
      " -4.8698   -0.83735   3.4817   -2.7028   -4.4796    2.5653    2.4439\n",
      " -1.326     2.1689    0.44454   2.6548    1.1941    0.77388  -1.0307\n",
      " -0.36114   0.76456   0.13615   0.60107  -1.9584   -1.8456    2.7714\n",
      " -0.40363  -2.0218   -2.7975    0.87308  -0.60881   0.54529  -0.83025\n",
      " -2.6085   -0.61694  -1.3153   -0.93897   1.2439    1.6728    3.6878\n",
      "  1.1282    0.76686  -0.65263  -0.38874  -5.5994   -1.7555    1.3363\n",
      "  2.3496    0.43585  -0.95383   0.13321  -1.9174   -0.18698  -0.42904\n",
      " -2.4861   -0.84611   1.4622   -1.0449    0.97269  -1.8023   -1.3815\n",
      "  1.2853   -0.31632  -0.5437    0.83663   2.0028   -2.0846   -1.0597\n",
      "  2.8311   -1.0626    0.28471   1.6699    2.5709   -1.3432    2.5363\n",
      "  1.8258   -3.5312   -0.36535  -1.491     1.5293    0.76608  -2.2002\n",
      " -3.0562   -2.1962    0.95726   2.4487   -0.57422  -2.5642    1.7715\n",
      " -0.82127  -3.1916    0.32214   2.2568   -2.7959   -0.39209  -2.1158\n",
      "  1.4973   -0.83545  -1.6194    1.3295    2.1082    0.14522  -5.5109\n",
      " -0.10648  -0.624    -2.8821    0.12921   0.3974    1.1959   -3.0543\n",
      "  0.58479  -2.5255    1.3405    0.66003   0.26409  -0.70738  -0.75407\n",
      " -1.237    -3.6562   -2.3117   -2.4754   -1.362     0.68819 ]\n",
      "------------------------------\n",
      "Token 2: 'está'\n",
      "Vector: [-2.9405   -1.5158   -0.53059  -2.8224   -0.52606  -2.3079    2.2148\n",
      "  5.0576    0.12305   0.056993  2.0674   -1.8243    9.6055   -2.0617\n",
      "  0.31184  -1.4651   -5.1057    5.768    -0.84432  -1.4762   -4.9172\n",
      " -1.3042   -5.1539   -3.8747    5.0032   -2.2987    2.299     2.4382\n",
      "  0.2706    7.0093    3.0159   -2.9293    2.4545    4.8941   -3.9328\n",
      " -2.1673    0.13243   3.4561    0.80677  -0.3791   -0.18971   3.7026\n",
      "  4.851     0.15034  -6.0026    5.2758    2.693    -1.65      4.3622\n",
      "  1.9096    1.0879   -0.54256   3.5753   -1.4637    1.8892    2.1309\n",
      "  1.6355   -3.6076   -2.7089   -0.26958  -1.5421   -1.6959   -1.3375\n",
      "  0.12881  -5.2201    2.2631   -4.3803    3.0279    1.6683   -1.8099\n",
      "  1.0053    3.2586   -2.3472    3.8227   -0.94368   1.6512    5.139\n",
      " -0.67632   1.4932    4.3571    0.57342  -2.411    -0.15211   6.2281\n",
      " -1.0753    3.1977    0.78951  -2.197    -1.6566    1.427    -3.2216\n",
      "  4.0222    0.43679   4.1564    0.69403  -0.97024  -5.2798   -1.9513\n",
      " -0.62406   2.7037   -1.9935    4.8714   -3.4747   -1.5693   -0.5421\n",
      " -2.2615    0.33502  -2.3786   -1.2087   -0.58435  -0.9965    0.51271\n",
      " -1.4995   -1.957     5.1802   -2.4055    0.59034   5.5499   -4.997\n",
      "  0.21331   5.7575    4.6141    2.5279   -2.0648    4.4656   -1.7647\n",
      "  2.2324   -3.4319    0.22065  -3.7189   -0.69082   0.68056   1.9067\n",
      " -2.958     0.33021  -0.86427  -0.60802   4.5096   -4.5173    0.91314\n",
      " -1.0167   -0.20895   0.18658   0.96196  -3.2277   -0.91657  -0.47692\n",
      " -0.87426   3.4988   -0.097376 -2.0534   -4.5882   -1.2748   -1.44\n",
      "  1.7177    1.8132   -4.6401   -3.7207   -2.9784   -1.2209    6.4216\n",
      "  0.55276   1.6823    0.96662   2.7298   -3.129     1.2422   -0.89629\n",
      "  3.637    -2.3248    6.2528    6.3102   -4.0192   -1.249     1.28\n",
      "  1.3367   -5.1649    1.4445   -4.3443   -0.47145  -1.189     0.67238\n",
      "  2.2301   -0.58495  -5.0442   -1.3583   -1.7778    0.88934   0.69069\n",
      "  1.0065    4.9411    1.905     0.22176   7.1113    1.9586   -1.315\n",
      " -4.9952    3.0233   -2.4452    3.4969    3.4301   -3.8689   -0.92278\n",
      " -2.4579   -5.1046   -1.7238   -0.51787   4.2263   -1.6626   -3.4249\n",
      "  2.8905   -0.27627   0.069809 -0.16756   1.8185   -1.1175    9.0676\n",
      "  3.8556   -1.1657    5.8677    0.074208  4.748    -3.716     3.5357\n",
      "  6.4342   -1.6049    2.6756   -6.5553    1.9669   -3.627     5.3242\n",
      "  0.15933  -0.31038   1.2382   -6.0941   -0.7013    2.5296   -3.9729\n",
      " -2.2394    3.8433    1.4151    1.5983   -4.6649   -5.3306   -3.2731\n",
      "  0.49084  -1.6081    4.157     5.6897    4.6429   -1.0455   -3.314\n",
      " -2.58      0.25204   1.3838    5.3085   -3.7214    2.6058    5.4877\n",
      " -3.0835   -2.8776   -0.57898  -0.74421  -0.48053   6.4905   -0.44552\n",
      "  0.50935  -2.5622   -0.69875  -0.34669   1.8406    4.1776    0.12778\n",
      " -0.58497   0.22227  -7.7609    5.9529   -1.1063   -5.9791   -0.27475\n",
      "  0.55867  -1.7785   -2.8085    2.8908    4.8731    1.6974   -0.70085\n",
      "  4.5627    4.7333   -5.099     0.29386  -0.72679  -3.3409   -0.41581\n",
      "  2.8167    1.2676   -3.8835    2.229    -2.4201   -0.63901 ]\n",
      "------------------------------\n",
      "Token 3: 'durmiendo'\n",
      "Vector: [ 1.1955    1.3132    1.1836   -1.1391    0.62439  -0.94475  -4.5947\n",
      "  0.12433   1.9948    2.3239   -0.69358  -0.010695  0.036825  0.38882\n",
      " -1.6359    0.10281  -1.2156    1.1703   -0.72163   0.71163   1.398\n",
      " -1.9882   -0.15341  -0.50541  -0.13844  -0.22956  -0.82734  -0.21105\n",
      " -0.39124   0.60241  -1.7395    2.2455    2.7734   -0.66855   0.28659\n",
      " -0.23849  -0.12495  -0.35107   0.8696    0.38132   2.2391    0.36197\n",
      " -0.63273   1.3917    1.1173   -0.7088   -2.4713   -1.5997    0.71405\n",
      "  0.5767    0.46429  -0.87925   0.42264   3.2152   -0.81638  -1.6746\n",
      " -0.093344 -0.67804   1.266     1.2579    0.33556   3.0083    0.78732\n",
      "  0.61921  -1.1733   -0.41513  -1.3454   -0.89562   0.65821  -0.43124\n",
      "  1.3092   -0.013771  0.13913   0.50847   0.71088   0.47964   1.0604\n",
      "  1.9308   -0.10211  -2.4273   -0.65037  -0.57139  -0.65999  -0.39587\n",
      " -2.8135    1.2813    0.52248  -0.059538 -0.24197  -1.8703   -0.165\n",
      "  2.18     -1.2053    0.75091  -0.93733  -0.40693  -2.3136   -0.87805\n",
      " -0.51505  -0.26413   0.43757   0.27225   0.25597   1.4707   -0.78622\n",
      "  1.605     0.82442   0.92826  -0.63283   1.4606    0.34119  -0.34693\n",
      "  0.88252  -1.6399   -0.097527  0.65008   0.8358    1.1267   -1.1735\n",
      " -0.15578  -0.80191  -0.80461   0.1829   -0.32471   0.67456  -0.078781\n",
      " -0.34539  -0.94143   3.9379    0.25346   0.69374   0.80962   0.60784\n",
      "  1.2858   -0.38447  -1.7236    0.30778   1.4642    0.17344   1.2386\n",
      " -0.3234   -1.501     0.20084   2.4655    0.068994  2.4575   -2.2144\n",
      " -0.7631   -1.6089    0.96839  -1.7256   -0.11475  -0.52412  -0.42687\n",
      "  0.23531   0.29066  -1.4323   -0.58864   0.51165  -0.6496   -1.0835\n",
      " -0.19776   0.46518   2.7012    0.87119   0.27141  -0.22262  -0.48136\n",
      "  1.5276    0.61898   0.41743   0.16077  -1.5147    0.44996  -0.042641\n",
      "  1.1561    0.95159   1.7752    2.2271    1.1601    0.64286   0.32948\n",
      " -2.8631   -1.7939   -1.8956   -0.24172  -1.3551    0.97136  -1.7861\n",
      " -0.99447   1.4098    0.61414  -1.4898   -0.76943   0.083877 -0.61214\n",
      " -0.69829  -0.48737   1.2139   -2.1147    0.747     0.03918  -1.6401\n",
      " -0.39178   0.74035   0.46127  -0.19239  -2.1049    3.2807    1.2373\n",
      " -1.4509    0.51124   0.85943  -0.75206   0.33551   0.073724  1.5407\n",
      "  1.4243   -0.066985  0.74281  -0.055426 -1.5027    0.19816   2.08\n",
      "  1.1955    0.34322   1.9207   -0.61564  -1.2409    1.1951   -0.86215\n",
      " -1.7601    0.78107   1.2023    0.39469  -0.57376  -0.5355    0.09785\n",
      "  1.6366   -0.13175  -0.008286 -0.74012  -1.6847   -0.41347   1.3864\n",
      "  0.52602  -1.2072    1.7996    0.23657  -0.80625  -0.52522   0.89048\n",
      " -0.46349  -1.6598   -1.1856    0.8737    0.59678   1.241    -1.3109\n",
      " -0.29357  -0.23113   0.020807 -0.20813  -1.7      -1.3809    0.40374\n",
      "  0.43492   0.65931   0.095386  1.5513    0.095126 -0.69795   0.94906\n",
      " -1.1573   -1.1682    0.79474   0.66273  -1.2261   -0.72099  -1.757\n",
      "  0.66205   1.4449    0.39362  -0.90572  -0.44606   0.74722   1.1226\n",
      "  0.51255   0.85182   0.17615   0.45767   0.95856  -0.55748   0.94536\n",
      " -0.32095  -2.6714    0.54833   0.032109 -1.2281    1.2291  ]\n",
      "------------------------------\n",
      "Token 4: 'en'\n",
      "Vector: [ 4.9883e+00 -3.2795e+00  6.7230e+00  2.2728e+00  2.5439e+00  2.3657e+00\n",
      " -2.8446e+00 -2.9669e+00 -1.6124e+00 -2.4400e-01 -7.9675e-01  3.6962e+00\n",
      "  3.2326e+00  5.9354e-01 -1.5080e+00  3.3693e+00 -2.0174e+00  1.2580e+00\n",
      " -1.3355e+00  3.3208e+00  2.7771e+00  7.1043e-01 -3.7797e-01  7.6289e+00\n",
      "  1.5009e+00  1.7779e+00 -1.4321e+00  4.4708e+00  4.1541e+00 -3.3874e+00\n",
      " -2.2730e+00  3.6735e-01  1.2376e+00  1.7083e+00  5.1391e+00 -3.7391e+00\n",
      "  2.1927e+00 -2.7297e+00  3.6093e+00 -1.5685e+00 -2.3963e-01  1.0045e+00\n",
      " -4.6404e+00  2.1289e+00 -7.5118e+00  3.4886e+00 -3.6659e+00 -2.2797e+00\n",
      " -4.6314e+00 -5.9122e+00  2.2967e+00  7.3062e-01  1.4148e+00  6.8835e+00\n",
      " -2.4233e+00 -3.5483e+00 -3.8618e+00  6.1565e+00 -3.7071e+00 -6.7299e-01\n",
      " -3.9333e+00  1.1869e-01  1.2990e+00 -4.6183e+00  5.6888e+00  3.9009e+00\n",
      "  1.9864e+00 -3.9734e+00  7.2836e-02 -7.6777e+00  6.4097e+00  7.6811e-01\n",
      " -1.5704e+00 -1.4718e+00  2.7376e+00  9.1731e+00 -6.5728e-01  8.2583e+00\n",
      "  1.5469e-01 -1.7861e-01 -4.5346e-01 -2.8815e+00 -4.6467e+00  7.8506e+00\n",
      " -1.0802e+00  2.3711e+00  6.1246e+00 -3.8255e+00  6.0735e+00 -5.8021e+00\n",
      " -5.1407e+00  2.0312e+00 -5.4498e+00  9.1787e-01 -1.0421e+00  9.2609e-01\n",
      "  6.5265e+00  6.4947e-02 -4.7168e+00  1.9550e+00  1.5097e+00  9.2629e-01\n",
      " -3.0587e+00 -3.5351e+00 -1.5751e+00 -3.6029e+00  8.6640e-01 -1.6213e+00\n",
      " -3.2638e+00  2.2345e+00 -7.3441e-01  1.6820e+00 -1.6222e+00 -3.7464e-01\n",
      " -4.6597e+00  4.9081e+00  2.1575e+00  6.2744e+00 -4.5690e+00  2.0187e+00\n",
      "  3.0227e+00 -1.6162e-01 -5.6336e+00  6.1663e-02  1.2462e-01 -6.6684e-02\n",
      "  2.3331e+00 -5.4604e-01 -9.1097e-01 -1.6572e+00  8.0613e-01 -1.1070e+01\n",
      " -3.4854e+00 -2.5672e+00  4.7495e+00 -1.6000e+00 -2.6949e+00  1.0527e+00\n",
      "  1.0323e+00  2.5618e+00  4.3839e+00  5.2496e+00  4.0300e+00  5.1768e+00\n",
      " -3.4235e+00 -4.5264e+00  2.2780e+00 -8.0612e+00 -1.8033e+00  3.2620e+00\n",
      " -3.9976e+00  1.6397e-02 -3.2693e+00  5.4052e+00  4.3385e+00 -9.7852e-01\n",
      " -9.4092e-01 -1.4996e+00 -5.7231e+00  3.0754e-01  7.0239e-01  3.3090e+00\n",
      "  3.8779e-01 -4.5456e+00  4.8930e+00  2.4100e+00 -4.2301e+00 -6.1802e+00\n",
      " -2.9639e-01 -9.9512e-01  2.7066e+00 -3.3099e+00 -2.1483e+00  7.6448e+00\n",
      "  2.5392e+00  1.1754e+00 -7.4318e+00 -8.7428e+00  9.0630e+00 -2.0507e+00\n",
      " -1.2006e-02 -2.8389e+00  4.0037e+00 -4.7381e-01 -2.3576e+00 -3.2249e+00\n",
      " -2.4325e+00  3.3137e+00 -3.7062e+00 -1.0889e+00  3.7907e+00 -5.5141e-01\n",
      " -2.7443e+00 -8.7846e-01  5.1123e+00  1.1270e+00  2.1957e+00  2.0053e+00\n",
      "  4.3077e+00 -6.1012e-03  6.1669e+00 -3.5609e+00 -1.8003e-01 -5.8695e+00\n",
      "  2.3206e+00  2.5580e-01  5.0270e+00  1.9213e-01  3.5819e+00  5.2704e-01\n",
      " -1.0412e+00 -2.6493e+00 -3.2293e-01 -4.2172e+00  1.9689e-01  2.6609e+00\n",
      "  1.9968e+00  7.2228e+00 -5.4025e+00 -9.0702e-01 -4.6858e+00 -2.1782e+00\n",
      "  8.1893e-01 -7.8039e-01  5.0080e+00 -2.2288e+00 -6.2499e+00  1.1841e+00\n",
      "  2.5175e+00  3.4848e+00 -2.4464e+00 -3.6411e-01  1.5544e+00  1.5224e+00\n",
      "  1.6988e+00 -8.9391e-01 -2.3614e+00  1.4358e+00  8.0438e-01  3.2500e+00\n",
      "  4.8509e+00  1.6288e+00 -7.3712e+00  1.5313e+00  3.3592e-01  1.8199e+00\n",
      "  3.3894e+00  4.2812e+00  1.2455e+00 -2.6688e-01  8.0606e+00 -2.6372e+00\n",
      " -1.0344e+00  6.4616e+00  8.1679e-01  4.4987e+00 -3.5762e+00 -4.0232e+00\n",
      " -1.6208e+00 -3.3979e+00  4.6891e+00  7.2302e+00 -4.3813e+00 -6.8243e-01\n",
      "  2.1528e+00 -3.4367e+00 -8.3364e-02  4.4355e+00  2.4232e+00  2.2159e+00\n",
      " -1.8376e+00 -1.4545e+00  7.3670e+00 -2.0814e+00 -3.5673e+00  3.8475e-01\n",
      "  2.6559e+00  4.3742e+00 -1.2350e+00 -6.0101e+00  1.0665e+00 -2.9782e+00\n",
      "  2.8718e+00  5.4602e+00  4.5645e+00  5.8434e+00  1.5971e+00 -1.8306e+00\n",
      "  4.7819e+00 -4.5849e+00 -1.7349e+00 -8.0675e-01  1.7984e+00  1.5537e+00\n",
      "  9.8697e+00  6.8328e-01  3.5657e+00  3.6367e+00  4.7615e+00 -6.9504e-01]\n",
      "------------------------------\n",
      "Token 5: 'el'\n",
      "Vector: [-1.6194e+00  1.1347e+01 -1.8904e+00  4.8450e+00 -1.2819e+00  1.2499e+00\n",
      "  1.1410e+00 -1.0732e+00 -3.9207e+00 -3.0300e+00  2.7747e+00 -2.6446e+00\n",
      "  1.4155e+00 -4.2990e+00 -4.4629e+00  3.9047e+00  6.6678e+00  5.1291e+00\n",
      " -9.1953e-01  5.8249e+00 -6.3079e+00 -2.8351e+00 -6.1279e+00  1.7411e+00\n",
      "  5.6919e-01 -1.2752e+00  1.0035e+01 -1.6350e+00 -5.5863e-01  1.5150e+00\n",
      " -4.3747e+00 -3.6658e+00 -6.0558e+00  2.3227e+00  2.5476e+00 -4.9973e+00\n",
      " -1.7275e+00  5.2920e+00  3.4221e+00 -4.0181e+00 -2.0203e+00  1.6255e+00\n",
      " -2.3406e+00 -6.4514e-01  2.8294e+00 -2.0511e+00  2.6727e+00 -3.5793e+00\n",
      " -1.1397e+00  1.1451e+00 -1.5736e-01 -9.8520e-01  4.1442e+00  2.2519e-01\n",
      "  1.0002e+01  9.6069e-01 -2.1536e+00 -7.7299e-01  2.1900e-01  3.9840e+00\n",
      " -7.6648e+00  1.6327e+00 -4.1159e-01 -3.3739e+00 -1.6712e+00 -1.5445e+00\n",
      " -4.1791e+00 -2.8420e+00 -4.2715e+00 -1.0546e+01 -2.2345e+00  3.0528e+00\n",
      " -3.6235e+00  5.0227e-01  1.4543e+00  6.9908e+00  3.1303e+00  2.9104e+00\n",
      " -1.8322e+00  4.0904e-01 -6.6042e+00 -1.3130e+00 -5.1803e+00  2.0358e+00\n",
      "  3.4134e+00 -1.5555e+00  4.8932e+00 -2.5680e+00  4.3675e+00  5.5869e+00\n",
      "  1.4950e+00  1.0575e+00 -3.5765e-01  1.1400e+00 -4.2757e+00  3.3750e-01\n",
      " -6.9376e+00 -6.4451e+00  8.0452e-01 -7.3848e+00  5.7876e+00 -7.3387e-01\n",
      " -3.4360e-01  3.7244e+00 -1.1423e+00  9.7336e-01  2.1388e+00 -2.8150e+00\n",
      " -1.5643e+00  5.4367e+00  2.0391e+00  3.8064e+00 -3.9247e+00 -3.3331e-01\n",
      " -5.9569e+00 -2.3028e+00  6.6694e+00  2.4108e+00  2.0240e+00 -3.4942e+00\n",
      "  4.3539e+00  5.1150e+00 -2.3659e+00 -5.1908e+00 -9.6141e-01  5.3662e+00\n",
      "  4.3520e+00  1.0616e+00 -1.2914e+00  2.7530e-01  3.3759e+00 -6.6851e+00\n",
      " -7.5348e+00 -4.1733e+00  3.4574e+00 -2.3301e+00  3.1126e+00 -2.8756e+00\n",
      "  5.6057e+00  3.8775e+00  5.7499e+00 -1.7386e+00 -6.2279e+00  2.8040e+00\n",
      "  1.4850e+00 -1.4534e+00 -5.2499e+00 -2.1933e+00 -9.0344e+00  3.8645e+00\n",
      " -2.7033e+00  1.9433e+00  4.8164e+00  2.0240e+00  4.1124e+00  1.6993e+00\n",
      " -1.9861e+00  6.7220e+00 -2.9288e+00 -9.1749e-01  1.0529e+00  2.2119e+00\n",
      "  3.0505e-02  1.0096e+00 -7.4927e-01 -4.7406e+00 -2.5343e+00  4.1691e+00\n",
      "  3.2231e+00 -2.1702e+00 -1.7250e+00 -1.6214e+00  4.2200e-01 -1.1267e+00\n",
      "  6.6108e+00 -1.1169e+00 -2.2596e+00 -6.6944e+00  1.3857e+00  4.0944e-01\n",
      "  3.2637e+00 -4.3699e+00 -1.3797e+00 -4.3116e+00  4.7802e+00  1.2648e+00\n",
      "  2.3711e+00  3.6250e+00 -5.5053e+00 -3.8540e+00 -1.3583e+00 -8.1370e+00\n",
      " -2.7850e+00 -4.1920e-01  1.1294e+01 -1.3872e+00 -5.2222e+00  5.9752e+00\n",
      " -2.6804e+00 -4.8475e-01 -1.2667e+00 -4.6246e+00 -1.8745e+00 -8.7154e-03\n",
      " -7.5343e+00  3.0278e+00  2.7208e+00  3.9271e+00  7.8610e+00 -6.0735e-01\n",
      " -5.9950e+00 -3.2069e+00  3.2893e+00 -3.6901e+00 -1.2611e+00  6.4825e+00\n",
      "  1.8334e+00  7.3638e-01 -7.4772e+00 -1.7596e+00 -4.0096e+00  2.5252e+00\n",
      "  2.0351e+00 -6.0539e+00  5.5407e+00  1.4168e+00 -4.7022e+00  7.9057e-01\n",
      " -3.1030e-01  8.3223e+00  4.0636e+00  8.4533e+00  3.0833e+00  6.7511e+00\n",
      "  9.5105e-01 -2.4921e+00  9.6078e-01  2.8673e-01  1.3869e+00  2.5674e+00\n",
      " -4.1305e+00 -1.2146e+00 -8.3519e+00  1.4055e+00 -7.1359e+00  4.4286e+00\n",
      " -9.2349e-01  1.9586e+00  3.9625e+00  2.3751e+00  4.7878e+00  3.5750e+00\n",
      "  2.1714e+00  6.6257e+00  1.2945e+01 -7.4724e+00 -2.0055e+00 -2.9313e+00\n",
      " -1.1392e+00  6.0554e-01  2.5712e+00  3.7972e+00 -2.0004e+00  4.3857e+00\n",
      "  2.5096e+00 -5.4046e+00 -7.8543e+00 -5.8777e+00  4.3577e+00  2.0738e+00\n",
      " -5.1361e+00 -3.4258e+00  5.9812e+00 -7.2061e-01  1.6959e+00 -8.5744e+00\n",
      "  1.4188e+00  6.9821e+00  5.5734e+00 -4.1414e+00 -1.1906e+00 -2.4569e+00\n",
      " -1.0163e-01  9.5856e-01  4.8782e+00  5.2247e+00 -8.0482e+00 -9.8634e+00\n",
      "  6.1902e+00  1.0616e+00  4.6732e+00 -2.9907e+00 -3.1376e+00 -3.6174e+00\n",
      " -1.3083e+00 -2.4310e+00  4.6911e+00  4.1889e+00  1.7969e+00  4.7222e-01]\n",
      "------------------------------\n",
      "Token 6: 'sofá'\n",
      "Vector: [-0.65411    0.55106    0.021176   0.83131    1.6995    -2.2187\n",
      " -3.1033    -0.38351    3.3062     1.8905    -0.58782   -2.0636\n",
      " -1.6824    -1.646     -0.86196   -3.3232    -3.3011    -1.2449\n",
      " -0.62503   -1.692      0.1137    -4.2629     0.02427    0.37451\n",
      "  0.1803    -0.11572   -1.1529     0.046283   1.0888     1.3572\n",
      " -2.6038     0.058138   1.9372    -1.0733    -2.9781     1.639\n",
      "  0.61562   -2.4765     0.75722   -0.026679   2.8437     0.60548\n",
      "  0.19024    2.5649     3.4384     1.1185    -0.99191    3.5567\n",
      "  2.5439    -3.649      0.82959    1.8284     0.096879   1.4925\n",
      "  0.36393   -2.5018     0.48036    0.84525   -0.62097    1.5334\n",
      "  3.5999     0.94409    1.189      2.3248    -2.5879    -1.1369\n",
      " -1.1494    -1.8218    -4.371     -0.49567    0.14319   -0.64397\n",
      "  1.8487    -1.5288    -0.47185   -1.2885     0.024417  -3.2441\n",
      "  0.85118   -5.2426     1.1476    -0.88001   -1.0544     1.1467\n",
      " -6.2796     2.4334     0.52355    1.9253    -0.45038   -0.41151\n",
      "  1.4353     1.442     -0.41884    0.15349    1.1253    -1.5487\n",
      " -2.3328    -3.2108     1.1511     0.37749   -4.6396    -1.1199\n",
      "  2.437      0.46006    0.089332   0.95449   -1.8005     1.7491\n",
      " -1.2151    -0.86989    0.76244    2.369      1.1241     0.040048\n",
      " -2.7229    -0.10479    2.9407    -1.3312    -7.7899     2.5924\n",
      "  1.946      0.32557    2.6947     1.4218    -1.3459     0.50915\n",
      " -0.71376   -0.91435    2.6919    -2.3798    -0.66179    1.0482\n",
      "  0.8236     1.3483     0.33856   -1.2678    -3.7386    -0.32805\n",
      "  0.064097   3.0139     2.2021     0.64789    1.599      0.58288\n",
      "  1.2027     4.2682    -1.2763    -0.80112   -2.8436    -0.74436\n",
      "  0.33051    1.3029     1.2747    -1.0843     1.2149     1.105\n",
      "  2.5471    -2.1632     0.27488   -2.8538    -2.3038     1.7706\n",
      "  2.9378     0.61876   -0.9392    -0.77954    0.2215    -0.7822\n",
      "  0.63244   -1.447      5.3117    -3.7404    -2.0251    -0.09762\n",
      " -0.34476    1.3784     2.2845     1.9252     0.49732    0.58573\n",
      "  0.89484    1.1861    -3.7898    -2.2959    -0.9655    -1.1943\n",
      " -0.76077    0.62491   -0.99583   -1.319      2.6366     1.631\n",
      "  0.79488   -0.92944   -0.26263    0.68083    0.60057   -0.89982\n",
      " -1.5425    -1.3489     0.85563    0.27583   -1.1708     1.3553\n",
      "  1.0854     3.0015    -1.6632    -0.0081422  2.3226    -2.5351\n",
      "  2.5337    -2.6536     1.3597    -1.8685    -0.7193    -2.1956\n",
      " -0.79032    2.5075     3.601     -2.3269    -2.2416    -4.4724\n",
      "  0.36904    1.8584    -0.15352   -0.82268   -1.7108    -2.5277\n",
      " -2.307     -1.0644     2.033     -3.3649    -2.1418     1.3704\n",
      " -1.0091     0.85464    1.591     -3.8235     4.7861     0.5095\n",
      "  0.71856   -1.639      0.73706   -0.51827   -1.6003     3.6881\n",
      " -1.5688    -1.78       3.861      0.91236   -3.7044     0.25536\n",
      "  3.284     -3.5518    -2.5382    -0.46544    0.64083   -0.97334\n",
      " -0.46558    0.32344   -1.6355     3.1464     0.50785    1.1945\n",
      " -0.36044    2.0099    -1.5395    -2.1006     0.13108    0.52052\n",
      " -0.34489    2.6085    -2.4693    -1.2671    -1.1772    -1.5138\n",
      " -0.32425   -4.7868    -1.2039    -1.9583    -0.16697    0.54221\n",
      " -3.371      1.095      3.1205     0.24344    2.4672    -0.89878\n",
      " -0.57754   -0.51651    1.637      1.09      -0.82929    0.96287\n",
      " -2.6116    -4.9473    -1.7793    -2.3696     0.8737     0.88675  ]\n",
      "------------------------------\n",
      "Token 7: '.'\n",
      "Vector: [ 1.6517   -1.9634   -0.60317  -1.4497   -4.6456   -2.6548    1.7871\n",
      "  2.0086    2.5595   -6.62     -3.1862    1.4241    1.2941    2.3816\n",
      " -2.2026    3.1901   -0.069615  3.4981    0.52599   2.1718    0.68467\n",
      " -0.89486  -2.184     1.1761   -0.067467  0.72529   2.7139    0.59838\n",
      "  2.4567   -8.4852   -1.5334    1.765     0.074411  2.7647   -0.011132\n",
      " -3.8116   -2.1176    1.4034   -3.6477    1.435     1.5079   -3.9402\n",
      " -2.6653   -2.438    -2.5748    0.78063  -4.2288   -5.5485    1.2281\n",
      "  3.5425    2.5387   -1.5244   -2.9497    5.9991   -0.013804  0.53404\n",
      " -1.8439    0.8998   -3.0015   -1.8224   -2.5184    1.1081   -2.1192\n",
      " -1.2046    1.045    -0.61409   1.0887   -2.6562   -0.41009  -2.9739\n",
      "  3.899     2.4347    0.74896   1.858     1.002     1.6324   -0.90548\n",
      "  6.6497    0.24717  -0.42803  -0.10038  -2.6809   -1.1805   -3.3625\n",
      "  2.1539   -0.10481   3.3057    3.112     5.7779   -5.0753   -2.127\n",
      "  2.1877   -6.2819   -1.6592   -4.0378    2.2019   -1.9772   -1.5248\n",
      " -3.5156   -3.1314    0.23034   0.89325   0.33181   1.6749    0.81533\n",
      " -0.88029   1.2501   -1.1932    2.5691    0.74317  -0.88583   1.0391\n",
      "  7.776    -3.162    -2.6456    4.8706    0.61319  -0.68245   1.7849\n",
      "  3.6145    1.0658   -2.9448    1.2607    2.7204    2.6704   -0.25994\n",
      " -1.2124   -0.13574   0.97574   0.10461  -2.35     -7.8595    0.17082\n",
      " -2.864    -0.43391   2.6371    0.86701  -1.6068    2.0415   -1.7224\n",
      "  1.2635    3.1676   -2.7371    4.5863   -1.063     0.68731  -0.6117\n",
      " -0.89254   2.0062    1.002    -0.16514   0.62088  -0.93454   1.5502\n",
      "  1.0427   -3.2031    2.3137    2.2351   -1.1178   -0.016491 -2.593\n",
      " -2.4076    1.5886    3.8658   -3.5407    2.3651   -0.1282   -2.2884\n",
      " -4.1526    1.5237   -1.6451    3.958    -2.5176    1.271     5.5891\n",
      " -0.073714  1.7153    7.2809    0.67752  -0.11758   1.2539   -2.1887\n",
      " -1.5756    2.315    -2.6269   -0.78974   1.0015    0.38805  -0.87827\n",
      "  2.229     0.4337    3.5694    0.77441   2.4085   -0.81812   2.577\n",
      "  0.17484  -5.6634    0.70201   1.3582    1.3136    1.7878    2.3947\n",
      " -0.053988  0.30324   0.8196    0.54558   4.4698   -0.57433  -1.4294\n",
      "  3.3957   -1.1672    1.2204   -0.30839   2.2115    3.1325    5.957\n",
      "  5.6923   -6.7601   -0.2131   -0.73857  -0.73549  -2.4655    2.1199\n",
      "  3.8113    0.60626  -0.5744   -0.39691  -0.49172   3.1223    2.1085\n",
      " -0.35509   0.76224   1.7729   -3.1049   -4.8747   -4.7791    1.6022\n",
      " -0.42757  -2.6162    0.51915  -2.0794   -0.24428  -0.1492    2.1878\n",
      "  4.5774   -2.5017    2.8651   -1.6572    1.9492    1.4551    0.46257\n",
      " -1.8519    4.2251    2.1565    3.3613   -1.3283    2.9527    1.9768\n",
      "  2.5539    1.4193    6.8043    2.6752   -2.2353   -0.17639   1.1861\n",
      "  0.9132    1.835     0.20752   1.6543    0.99623  -1.8868    1.8383\n",
      " -1.7074    0.28406   4.3414   -0.044696  1.4921    0.49855   1.0152\n",
      "  2.1297    0.19676   2.6029    3.414     2.5571    5.8898    1.7887\n",
      "  3.6939    3.3534   -2.6737   -2.9593    0.7448    2.1898    0.79826\n",
      "  2.6447   -1.9847   -3.3541   -0.39062  -1.8326   -3.0347  ]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar un modelo de spaCy que incluya vectores de palabras (por ejemplo, es_core_news_md)\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Procesar la oración\n",
    "sentence1 = nlp(\"El gato está durmiendo en el sofá.\")\n",
    "\n",
    "# Iterar sobre los tokens de la oración\n",
    "for i, token in enumerate(sentence1):\n",
    "    # Obtener el vector del token actual\n",
    "    vector = token.vector\n",
    "    \n",
    "    # Imprimir el índice del token, el texto del token y su vector\n",
    "    print(f\"Token {i}: '{token.text}'\")\n",
    "    print(f\"Vector: {vector}\")\n",
    "    print(\"-\" * 30)  # Separador para mejor legibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4d2bdad-c52b-47fa-a57f-b3236a2c9d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre 'gato' y 'perro': 0.8487285375595093\n"
     ]
    }
   ],
   "source": [
    "# Calcular similitud entre palabras\n",
    "palabra1 = nlp(\"gato\")[0]\n",
    "palabra2 = nlp(\"perro\")[0]\n",
    "similitud = palabra1.similarity(palabra2)\n",
    "print(f\"Similitud entre 'gato' y 'perro': {similitud}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfa61a6e-6416-4862-914f-363a823cefa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reina: [ 8.4734988e-01  2.0150199e+00 -2.6791000e+00  1.5409999e+00\n",
      "  3.5159001e+00 -4.7280498e+00 -3.1292000e+00  3.8007998e+00\n",
      "  5.7187998e-01  4.0989971e-01 -3.4295001e+00 -6.6147399e+00\n",
      " -2.5922000e+00 -1.3039399e+00 -1.8069801e+00 -4.7517004e+00\n",
      " -2.8046999e+00 -1.9334999e+00 -1.9435000e-01 -7.3728991e-01\n",
      " -6.5071993e+00  7.8249991e-01  3.6840699e+00 -8.8590997e-01\n",
      "  1.9055300e+00 -1.2842098e-01 -4.4302602e+00 -4.4542699e+00\n",
      "  1.8649203e-01  1.1092999e+00 -2.3755901e+00 -5.6673999e+00\n",
      " -9.8401308e-03 -5.1597996e+00 -3.5953002e+00 -6.2658024e-01\n",
      " -3.9818001e+00  2.7628403e+00  7.9680004e+00 -1.7857900e+00\n",
      " -2.3445799e+00 -2.1208000e+00 -3.9808989e+00  1.0536799e+00\n",
      " -1.8299001e+00  2.1223998e-01 -3.9682007e-01 -6.1988001e+00\n",
      " -5.3381979e-01 -5.5203900e+00  5.1817899e+00 -5.4720001e+00\n",
      "  5.1661998e-01 -1.5167000e+00  5.0890002e+00  1.2207501e+00\n",
      " -2.2541599e+00 -7.7214994e+00  3.8038502e+00  6.0773001e+00\n",
      "  2.5643501e+00  3.1029999e+00  4.3465004e+00  3.5046799e+00\n",
      "  1.6331300e+00  2.8216200e+00 -1.7271399e+00 -3.3525500e+00\n",
      "  1.3784931e+00  1.6686580e+00 -2.2914503e+00 -2.0629601e+00\n",
      " -9.4510031e-01  5.8067999e+00 -4.5591197e+00  7.3741002e+00\n",
      " -7.2421398e+00  3.8428900e+00 -9.1464996e-01 -7.9478002e-01\n",
      " -2.4804339e+00 -6.6062002e+00  2.3559004e-01 -6.1636567e+00\n",
      " -1.3914901e+00 -1.2816269e+00 -1.1611400e+00 -5.8867991e-01\n",
      " -2.3081398e+00  4.4213986e-01 -4.0431900e+00 -4.2345200e+00\n",
      " -2.0104799e+00  1.6510792e+00 -6.9949970e-02  5.5610800e+00\n",
      " -9.4602990e-01 -4.7755003e+00  2.7586601e+00  4.8508000e-01\n",
      "  1.1125649e+00 -1.9797001e+00 -3.2074199e+00  3.2349992e-01\n",
      "  6.9199800e-02 -1.2368004e+00  3.7566700e+00  1.0297599e+00\n",
      "  3.2633001e-01  1.2784001e+00  6.6146402e+00 -2.3698399e+00\n",
      " -7.8772008e-01 -8.6399555e+00  2.2499001e-01  2.7603998e+00\n",
      " -4.5158000e+00 -3.8453102e+00  4.4703002e+00  5.1638002e+00\n",
      "  2.1178300e+00 -2.8102608e+00 -3.5604300e+00  4.4140201e+00\n",
      "  1.4483800e+00 -4.1677399e+00  7.8610992e-01  5.6578302e+00\n",
      "  4.1941004e+00 -2.2205997e-01 -5.2185965e-01 -4.9608893e+00\n",
      "  1.8876400e+00  1.1486499e+00  6.0689986e-02 -1.1902150e+00\n",
      "  2.3641002e-01 -8.8224995e-01  3.7658000e+00  7.2680006e+00\n",
      " -2.7569991e-01 -2.8201702e+00 -7.2884598e+00 -5.9623399e+00\n",
      " -5.7120991e-01 -1.7963099e+00 -8.4045994e-01  2.8686922e+00\n",
      "  2.7516000e+00  6.8369997e-01  8.9334202e+00 -4.1338401e+00\n",
      "  8.5657299e-01  1.8011298e+00  2.1259000e+00  3.8528497e+00\n",
      " -4.2231002e+00  4.0825000e+00 -1.0519500e+00  1.7031100e+00\n",
      "  2.2202101e+00  4.2657003e+00  4.7939200e+00  9.5969999e-01\n",
      " -6.6544008e+00  3.0396998e+00  2.9629703e+00 -2.4771700e+00\n",
      "  1.5583298e+00 -5.9155002e+00 -3.2797003e-01  3.2368000e+00\n",
      " -2.0848999e+00  2.5831800e+00  1.3876009e-01 -3.5572997e-01\n",
      " -1.9408699e+00  2.1352999e+00  8.8999271e-03  2.9591498e+00\n",
      " -1.5162401e+00 -3.8720012e-01 -4.9797001e+00  1.3837699e+00\n",
      " -3.1188502e+00  5.7879996e-01 -1.7343999e+00 -2.6162000e+00\n",
      " -6.8204002e+00 -2.2232199e+00  9.7039998e-01  6.6913004e+00\n",
      "  3.5184903e+00 -9.3761003e-01  3.1918702e+00  1.1183200e+00\n",
      " -6.0881705e+00  6.6957002e+00 -9.6299887e-02  4.9126296e+00\n",
      "  3.5879993e-01 -4.5356998e+00  1.7587399e+00 -6.3930011e-01\n",
      "  6.5237999e+00  6.6967988e-01 -2.8007400e+00 -6.9979601e+00\n",
      "  4.5306396e+00 -2.2095003e+00 -2.1733999e-01 -1.6471858e+00\n",
      " -3.1554008e-01  2.9352100e+00 -1.2925999e+00  6.3923001e+00\n",
      "  2.1567199e+00  1.1749001e+00  6.2429600e+00  3.4218001e-01\n",
      " -5.4503002e+00 -7.3627005e+00  1.9069999e+00 -2.7873399e+00\n",
      " -4.2569499e+00 -7.2010803e-01 -3.4437996e-01  8.5909992e-01\n",
      " -1.8489981e-01  2.0398002e+00 -2.2042899e+00  3.0594800e+00\n",
      " -2.8288000e+00 -4.4369001e+00  8.3655000e-01  2.9200602e+00\n",
      " -7.9295999e-01  5.7244301e+00 -2.6336899e+00 -1.8343000e+00\n",
      "  1.6980004e-01  5.7104003e-01 -3.1595998e+00  9.9945009e-01\n",
      " -2.3175979e+00  5.5106997e-01  1.3448900e+01 -3.0614600e+00\n",
      " -8.8908005e-01 -5.0381002e+00  2.6128902e+00 -2.0219002e+00\n",
      "  3.3103900e+00 -9.3601999e+00  3.0444303e+00 -2.4096098e+00\n",
      " -4.2645102e+00  2.3295701e+00 -2.6586699e+00 -1.1155000e+00\n",
      "  2.9406400e+00 -1.3576099e+00  1.4744999e+00  1.6292000e+00\n",
      " -4.7639151e+00  4.1092696e+00  1.4516100e+00  1.1900200e+00\n",
      "  2.2478440e+00 -6.1533010e-01 -1.6798973e-01  5.7437000e+00\n",
      " -2.0876999e+00 -2.7224998e+00 -2.0095899e+00  8.1272000e-01\n",
      "  9.7660613e-01 -2.7695000e+00 -2.5661016e-01 -5.0337000e+00\n",
      " -1.0036200e+00 -5.3624001e+00 -7.7353497e+00 -2.1193600e+00\n",
      " -1.0100007e-01 -2.9096999e+00  1.3389003e-01 -4.5225196e+00\n",
      "  4.1644797e+00 -2.7781000e+00  7.1388698e+00 -2.8873203e+00\n",
      " -1.5748600e+00  7.0537572e+00  2.8745000e+00  4.2109399e+00\n",
      "  1.1435802e+00 -6.9747007e-01 -3.0300503e+00  5.6053295e+00]\n"
     ]
    }
   ],
   "source": [
    "# Realizar operaciones semánticas\n",
    "rey = nlp(\"rey\")[0]\n",
    "hombre = nlp(\"hombre\")[0]\n",
    "mujer = nlp(\"mujer\")[0]\n",
    "reina = rey.vector - hombre.vector + mujer.vector\n",
    "print(f\"Reina: {reina}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6894d255-a651-49df-a9ac-89adfb78376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# Verificar si un modelo tiene vectores\n",
    "print(nlp.meta[\"vectors\"][\"width\"])  # Devuelve la dimensión de los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a19c5b7-3993-4c82-9d00-6015123702f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.8015960629076846\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargamos el modelo de lenguaje de tamaño mediano (md), que incluye vectores de palabras\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Es recomendable usar \"md\" o \"lg\" porque tienen embeddings\n",
    "\n",
    "# Procesamos dos textos con el modelo NLP\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")  # Convierte el texto en un objeto spaCy\n",
    "doc2 = nlp(\"Fast food tastes very good.\")  # Otro objeto spaCy con otra frase\n",
    "\n",
    "# Calculamos la similitud semántica entre los dos documentos\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc845698-1085-4e59-a144-ceb6752b0632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salty fries <-> hamburgers 0.5733411312103271\n"
     ]
    }
   ],
   "source": [
    "french_fries = doc1[2:4]  # Se selecciona un span (subsección del texto), en este caso, \"salty fries\"\n",
    "burgers = doc1[5]  # Se selecciona el token \"hamburgers\"\n",
    "\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ec4d22-7fa0-41a5-8db0-9d6dc8e6258d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1d147294550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crea un objeto nlp sin un modelo preentrenado, solo con el idioma inglés (\"en\").\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Agrega el componente sentencizer al pipeline de procesamiento.\n",
    "# El sentencizer detecta oraciones basándose en puntuación (. ! ?) sin depender de análisis gramatical ni aprendizaje profundo.\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc9455f-dc6e-4ec1-a9e2-3474b6e50636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Importa la librería requests para hacer solicitudes HTTP\n",
    "from bs4 import BeautifulSoup  # Importa BeautifulSoup para procesar HTML\n",
    "\n",
    "# Realiza una solicitud GET a la URL que contiene el texto de Shakespeare\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "\n",
    "# Extrae el contenido de la respuesta HTTP y lo procesa con BeautifulSoup\n",
    "# .text obtiene el texto puro eliminando etiquetas HTML\n",
    "soup = BeautifulSoup(s.content, \"html.parser\").text \n",
    "\n",
    "# Reemplaza \"-\\n\" (saltos de línea con guion) por una cadena vacía para unir palabras correctamente\n",
    "# Luego reemplaza los saltos de línea \"\\n\" por espacios para obtener un texto continuo\n",
    "soup = soup.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "# Define el tamaño máximo de procesamiento en SpaCy para evitar errores de límite de texto\n",
    "nlp.max_length = 5278439  # Establece el límite en 5,278,439 caracteres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecf0fd53-02eb-4044-af7f-071f247f88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de oraciones: 94134\n",
      "CPU times: total: 6.17 s\n",
      "Wall time: 6.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Procesar el texto con spaCy\n",
    "doc = nlp(soup)\n",
    "\n",
    "# Contar el número de oraciones\n",
    "print(\"Número de oraciones:\", len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55970fee-daf0-4c14-bbaa-1298e5c82557",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Aumenta el tamaño máximo de procesamiento de texto para evitar errores con textos largos\n",
    "nlp2.max_length = 5278439  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86814ff8-e13f-4d97-bb65-2e3c7aee3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de oraciones: 92067\n",
      "CPU times: total: 2min 50s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Procesa el texto almacenado en `soup`\n",
    "doc = nlp2(soup)\n",
    "\n",
    "# Cuenta la cantidad de oraciones detectadas en el documento procesado\n",
    "print(\"Número de oraciones:\", len(list(doc.sents)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e13bff32-6e79-424d-b6d2-ba46b7a81622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analiza los componentes del pipeline de procesamiento de `nlp2`\n",
    "# Esto muestra información sobre los componentes del modelo (como tokenización, lematización, etc.)\n",
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a72631eb-469f-4dd0-bb8b-249c084145e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotham PERSON\n",
      "John Wick PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy  \n",
    "\n",
    "# Carga el modelo pequeño de spaCy en inglés (\"en_core_web_sm\"), que incluye análisis gramatical y reconocimiento de entidades\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "# Texto de ejemplo que contiene nombres de lugares y referencias históricas\n",
    "text = \"Gotham was referenced in John Wick.\"  \n",
    "\n",
    "# Procesa el texto con spaCy, creando un objeto `doc` que almacena tokens, entidades y más\n",
    "doc = nlp(text)  \n",
    "\n",
    "# Extrae y muestra las entidades reconocidas en el texto\n",
    "for ent in doc.ents:  \n",
    "    print(ent.text, ent.label_)  # Imprime el texto de la entidad y su etiqueta de tipo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c09abf0-01bd-4005-895e-f6b50c973ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gotham GPE\n",
      "John Wick FILM\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Carga el modelo preentrenado \"en_core_web_sm\" para procesamiento en inglés.\n",
    "\n",
    "text = \"Gotham was referenced in John Wick.\"  # Texto de entrada.\n",
    "\n",
    "# Crear un EntityRuler para reconocer entidades específicas\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")  \n",
    "# before=\"ner\": Esto asegura que el EntityRuler se ejecute antes del componente preentrenado de ner (reconocimiento de entidades nombradas) en el pipeline, \n",
    "# lo que permite que las reglas personalizadas se apliquen primero y no sean sobrescritas por el modelo preentrenado.\n",
    "\n",
    "# Lista de entidades y sus patrones\n",
    "patterns = [\n",
    "    {\"label\": \"GPE\", \"pattern\": \"Gotham\"},  # Definir \"Gotham\" como una GPE (entidad geopolítica).\n",
    "    {\"label\": \"FILM\", \"pattern\": \"John Wick\"}\n",
    "]\n",
    "# Definimos un patrón para el EntityRuler. Aquí, \"label\" indica el tipo de entidad (GPE para Geopolitical Entity),\n",
    "# y \"pattern\" es el texto que queremos reconocer como dicha entidad, en este caso \"Gotham\".\n",
    "# GPE es una etiqueta para entidades geopolíticas, como países, ciudades o pueblos.\n",
    "\n",
    "\n",
    "# Se agregan los patrones de entidades al EntityRuler. El EntityRuler luego buscará este patrón durante el procesamiento del texto.\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# El texto se procesa utilizando el pipeline de spaCy. El objeto \"doc\" contiene toda la información procesada, incluyendo \n",
    "# las entidades reconocidas, las relaciones entre palabras, la sintaxis y más.\n",
    "doc = nlp(text)  \n",
    "\n",
    "# Para cada entidad, imprimimos su texto (lo que fue reconocido como entidad) y la etiqueta asociada (como GPE).\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4d1bf03-6e63-4366-a479-46276afc0bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'entity_ruler': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumen de los componentes (pipes) que forman parte del pipeline de procesamiento de texto en el modelo cargado. \n",
    "# Este método te proporciona información detallada sobre los diferentes componentes y su funcionamiento, incluidos los nombres de los pipes, su tipo y si están habilitados o no.\n",
    "nlp.analyze_pipes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe17cf68-d963-4355-8496-949892c81090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555) 555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "import spacy  \n",
    "\n",
    "text = \"This is a sample number (555) 555-5555.\"  \n",
    "\n",
    "# Crea un objeto de procesamiento de texto de spaCy usando un modelo vacío para inglés (sin ningún modelo preentrenado).\n",
    "# Esto es útil cuando queremos crear un pipeline completamente personalizado.\n",
    "nlp = spacy.blank(\"en\")  \n",
    "\n",
    "# Añade un componente \"entity_ruler\" al pipeline de spaCy. Este componente permite la creación de reglas para \n",
    "# detectar entidades personalizadas en el texto según patrones específicos.\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")  \n",
    "\n",
    "# Lista de entidades y patrones (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "    {\"label\": \"PHONE_NUMBER\", \"pattern\": [\n",
    "        {\"ORTH\": \"(\"},          # Coincide con el símbolo de apertura de paréntesis '('.\n",
    "        {\"SHAPE\": \"ddd\"},       # Coincide con tres dígitos (por ejemplo, \"555\").\n",
    "        {\"ORTH\": \")\"},          # Coincide con el símbolo de cierre de paréntesis ')'.\n",
    "        {\"SHAPE\": \"ddd\"},       # Coincide con tres dígitos (por ejemplo, \"555\").\n",
    "        {\"ORTH\": \"-\", \"OP\": \"?\"},  # Coincide con el guion '-' de manera opcional.\n",
    "        {\"SHAPE\": \"dddd\"}       # Coincide con cuatro dígitos (por ejemplo, \"5555\").\n",
    "    ]}\n",
    "]\n",
    "# En este fragmento se definen los patrones de coincidencia que se utilizarán para reconocer las entidades.\n",
    "# El patrón busca un número de teléfono en el formato \"(xxx) xxx-xxxx\".\n",
    "# - \"ORTH\": \"()\" coincide con los paréntesis.\n",
    "# - \"SHAPE\": \"ddd\" coincide con una secuencia de tres dígitos.\n",
    "# - \"OP\": \"?\" significa que el guion \"-\" es opcional.\n",
    "# Este patrón está diseñado específicamente para encontrar números de teléfono con el formato mostrado en el ejemplo.\n",
    "\n",
    "# Añade los patrones definidos al \"entity_ruler\". Esto asegura que el `entity_ruler` busque estos patrones en el texto\n",
    "# y los etiquete como entidades de tipo \"PHONE_NUMBER\".\n",
    "ruler.add_patterns(patterns)  \n",
    "\n",
    "doc = nlp(text)  \n",
    "\n",
    "# Para cada entidad, imprime su texto (lo que se reconoció) y su etiqueta asociada.\n",
    "for ent in doc.ents:  \n",
    "    print(ent.text, ent.label_)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d5292-e8a5-4cb1-a91f-07c15a533783",
   "metadata": {},
   "source": [
    "## Atributos Usados por el Matcher de spaCy\n",
    "\n",
    "El Matcher de spaCy es una herramienta poderosa para buscar patrones en textos basados en atributos específicos de los tokens. A continuación se describen los atributos más comunes que puedes utilizar:\n",
    "\n",
    "1. **`ORTH`**: Representa el texto exacto del token (como una cadena de texto).\n",
    "   - Ejemplo: Si el token es \"Hola\", `ORTH` será \"Hola\".\n",
    "\n",
    "2. **`TEXT`**: Similar a `ORTH`, representa el texto exacto del token.\n",
    "   - Ejemplo: Si el token es \"Hola\", `TEXT` será \"Hola\".\n",
    "\n",
    "3. **`LOWER`**: Representa el texto del token en minúsculas.\n",
    "   - Ejemplo: Si el token es \"Hola\", `LOWER` será \"hola\".\n",
    "\n",
    "4. **`LENGTH`**: Representa la longitud del texto del token (como un entero).\n",
    "   - Ejemplo: Si el token es \"Hola\", `LENGTH` será 4.\n",
    "\n",
    "5. **`IS_ALPHA`**: Es True si el token contiene solo caracteres alfabéticos.\n",
    "   - Ejemplo: Para \"Hola\", `IS_ALPHA` es True; para \"123\", es False.\n",
    "\n",
    "6. **`IS_ASCII`**: Es True si el token contiene solo caracteres ASCII.\n",
    "   - Ejemplo: Para \"Hola\", `IS_ASCII` es True; para \"ñ\", es False.\n",
    "\n",
    "7. **`IS_DIGIT`**: Es True si el token contiene solo dígitos.\n",
    "   - Ejemplo: Para \"123\", `IS_DIGIT` es True; para \"Hola\", es False.\n",
    "\n",
    "8. **`IS_LOWER`**: Es True si el texto del token está en minúsculas.\n",
    "   - Ejemplo: Para \"hola\", `IS_LOWER` es True; para \"Hola\", es False.\n",
    "\n",
    "9. **`IS_UPPER`**: Es True si el texto del token está en mayúsculas.\n",
    "   - Ejemplo: Para \"HOLA\", `IS_UPPER` es True; para \"Hola\", es False.\n",
    "\n",
    "10. **`IS_TITLE`**: Es True si el texto del token está en formato de título (primera letra en mayúscula).\n",
    "    - Ejemplo: Para \"Hola\", `IS_TITLE` es True; para \"hola\", es False.\n",
    "\n",
    "11. **`IS_PUNCT`**: Es True si el token es un signo de puntuación.\n",
    "    - Ejemplo: Para \",\", `IS_PUNCT` es True; para \"Hola\", es False.\n",
    "\n",
    "12. **`IS_SPACE`**: Es True si el token es un espacio en blanco.\n",
    "    - Ejemplo: Para \" \", `IS_SPACE` es True; para \"Hola\", es False.\n",
    "\n",
    "13. **`IS_STOP`**: Es True si el token es una palabra de parada (stopword).\n",
    "    - Ejemplo: Para \"el\", `IS_STOP` es True; para \"Hola\", es False.\n",
    "\n",
    "14. **`IS_SENT_START`**: Es True si el token es el inicio de una oración.\n",
    "    - Ejemplo: Para el primer token de una oración, `IS_SENT_START` es True.\n",
    "\n",
    "15. **`LIKE_NUM`**: Es True si el token se parece a un número.\n",
    "    - Ejemplo: Para \"123\", `LIKE_NUM` es True; para \"Hola\", es False.\n",
    "\n",
    "16. **`LIKE_URL`**: Es True si el token se parece a una URL.\n",
    "    - Ejemplo: Para \"https://example.com\", `LIKE_URL` es True.\n",
    "\n",
    "17. **`LIKE_EMAIL`**: Es True si el token se parece a un correo electrónico.\n",
    "    - Ejemplo: Para \"example@example.com\", `LIKE_EMAIL` es True.\n",
    "\n",
    "18. **`SPACY`**: Es True si el token tiene un vector asociado en el modelo de spaCy.\n",
    "    - Ejemplo: Depende del modelo utilizado.\n",
    "\n",
    "19. **`POS`**: Representa la etiqueta de parte de speech (POS) del token.\n",
    "    - Ejemplo: Para \"corre\", `POS` podría ser \"VERB\".\n",
    "\n",
    "20. **`TAG`**: Representa la etiqueta gramatical detallada del token.\n",
    "    - Ejemplo: Para \"corre\", `TAG` podría ser \"VBZ\".\n",
    "\n",
    "21. **`MORPH`**: Contiene información morfológica del token (género, número, tiempo, etc.).\n",
    "    - Ejemplo: Para \"corre\", `MORPH` podría ser \"VerbForm=Fin\".\n",
    "\n",
    "22. **`DEP`**: Representa la relación de dependencia sintáctica del token.\n",
    "    - Ejemplo: Para \"corre\", `DEP` podría ser \"ROOT\".\n",
    "\n",
    "23. **`LEMMA`**: Representa la forma base o lema del token.\n",
    "    - Ejemplo: Para \"corre\", `LEMMA` es \"correr\".\n",
    "\n",
    "24. **`SHAPE`**: Representa la forma del texto del token (por ejemplo, \"Xxxx\" para \"Hola\").\n",
    "    - Ejemplo: Para \"Hola\", `SHAPE` es \"Xxxx\".\n",
    "\n",
    "25. **`ENT_TYPE`**: Representa el tipo de entidad nombrada del token (si es parte de una entidad).\n",
    "    - Ejemplo: Para \"Juan\", `ENT_TYPE` podría ser \"PER\".\n",
    "\n",
    "26. **`_`**: Permite definir atributos personalizados (un diccionario de valores).\n",
    "    - Ejemplo: Puedes definir un atributo personalizado como `{\"_\": {\"es_verbo\": True}}`.\n",
    "\n",
    "27. **`OP`**: Es un operador para especificar la frecuencia de un patrón (por ejemplo, ?, +, *).\n",
    "    - Ejemplo: `{\"OP\": \"?\"}` significa que el patrón es opcional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75b93a2d-16bc-4838-ab52-14b2871a8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)]\n",
      "EMAIL_ADDRESS\n",
      "wmattingly@aol.com\n"
     ]
    }
   ],
   "source": [
    "import spacy  \n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "\n",
    "# Creamos un objeto Matcher y le pasamos el vocabulario del modelo cargado\n",
    "# Matcher permite definir reglas personalizadas para buscar patrones dentro del texto.\n",
    "matcher = Matcher(nlp.vocab)  \n",
    "\n",
    "# Definimos un patrón que busca coincidencias con direcciones de correo electrónico\n",
    "# La clave \"LIKE_EMAIL\" es un atributo booleano que spaCy usa para identificar estructuras con formato de email.\n",
    "pattern = [{\"LIKE_EMAIL\": True}]  \n",
    "\n",
    "# Agregamos el patrón al matcher con la etiqueta \"EMAIL_ADDRESS\"\n",
    "# Aquí, \"EMAIL_ADDRESS\" es un identificador para la regla, y [pattern] es la lista de patrones que queremos que reconozca.\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])  \n",
    "\n",
    "doc = nlp(\"This is an email address: wmattingly@aol.com\")  \n",
    "\n",
    "# Aplicamos el matcher al documento procesado\n",
    "matches = matcher(doc) \n",
    "\n",
    "print(matches)\n",
    "# - El ID del patrón coincidente en el vocabulario de spaCy\n",
    "# - La posición de inicio del match dentro del texto\n",
    "# - La posición de fin del match dentro del texto\n",
    "\n",
    "print (nlp.vocab[matches[0][0]].text)\n",
    "# Esto accede al vocabulario de spaCy y usa el ID del patrón encontrado para recuperar su nombre.\n",
    "# En este caso, la salida será \"EMAIL_ADDRESS\", que es el identificador que le dimos al patrón.\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]  # Extraemos el texto coincidente\n",
    "    print(matched_span.text)  # Imprimimos la dirección de correo encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ff2175d-79bc-4675-89bd-bf33ace085eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coincidencia encontrada: Hola, mundo.\n",
      "Coincidencia encontrada: Hola, amigos.\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Definir un patrón basado en atributos\n",
    "pattern = [\n",
    "    {\"LOWER\": \"hola\"},  # Token en minúsculas\n",
    "    {\"IS_PUNCT\": True},  # Signo de puntuación\n",
    "    {\"POS\": \"NOUN\"},     # Sustantivo\n",
    "    {\"IS_PUNCT\": True}  # Signo de puntuación\n",
    "]\n",
    "\n",
    "# Añadir el patrón al Matcher\n",
    "matcher.add(\"SALUDO\", [pattern])\n",
    "\n",
    "# Procesar un texto\n",
    "doc = nlp(\"Hola, mundo. Hola, amigos. Adios, amigos.\")\n",
    "\n",
    "# Buscar coincidencias\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(f\"Coincidencia encontrada: {doc[start:end].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d1963b3-0633-4aa0-aa2b-d988ed357d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "(3232560085755078826, 0, 1) Sigmund\n",
      "(3232560085755078826, 1, 2) Freud\n",
      "(3232560085755078826, 3, 4) /frɔɪd/\n",
      "(3232560085755078826, 4, 5) FROYD\n",
      "(3232560085755078826, 12, 13) Sigismund\n",
      "(3232560085755078826, 13, 14) Schlomo\n",
      "(3232560085755078826, 14, 15) Freud\n",
      "(3232560085755078826, 17, 18) May\n",
      "(3232560085755078826, 21, 22) September\n",
      "(3232560085755078826, 72, 73) Freud\n"
     ]
    }
   ],
   "source": [
    "with open(\"freud.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Creamos una instancia de Matcher que nos permitirá buscar patrones en el texto\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Definimos el patrón de búsqueda: buscamos los sustantivos propios (PROPN)\n",
    "# El patrón es una lista de diccionarios que definen los atributos que estamos buscando en los tokens.\n",
    "pattern = [{\"POS\": \"PROPN\"}]  # 'POS' se refiere a la parte del habla (Part of Speech), en este caso buscamos 'PROPN' (sustantivos propios)\n",
    "\n",
    "# Añadimos el patrón al matcher con un nombre único para identificarlo\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])  # \"PROPER_NOUNS\" es el nombre del patrón, y [pattern] es la lista con el patrón de búsqueda\n",
    "\n",
    "# Procesamos el texto con el modelo 'nlp', creando un objeto doc que contiene todos los tokens procesados\n",
    "doc = nlp(text)\n",
    "\n",
    "# Ejecutamos el matcher sobre el 'doc' para encontrar todas las coincidencias del patrón definido\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Imprimimos el número total de coincidencias encontradas\n",
    "print(len(matches))\n",
    "\n",
    "# Iteramos sobre las primeras 10 coincidencias para imprimir los detalles\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])  # Imprime el identificador del patrón y el fragmento de texto correspondiente al match\n",
    "    # 'match' es una tupla con el formato (id_patron, inicio_token, fin_token)\n",
    "    # 'doc[match[1]:match[2]]' muestra los tokens entre los índices 'match[1]' y 'match[2]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6fbbd04-d861-442b-a3dd-f962811d29ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "(3232560085755078826, 0, 1) Sigmund\n",
      "(3232560085755078826, 0, 2) Sigmund Freud\n",
      "(3232560085755078826, 1, 2) Freud\n",
      "(3232560085755078826, 3, 4) /frɔɪd/\n",
      "(3232560085755078826, 3, 5) /frɔɪd/ FROYD\n",
      "(3232560085755078826, 4, 5) FROYD\n",
      "(3232560085755078826, 12, 13) Sigismund\n",
      "(3232560085755078826, 12, 14) Sigismund Schlomo\n",
      "(3232560085755078826, 13, 14) Schlomo\n",
      "(3232560085755078826, 12, 15) Sigismund Schlomo Freud\n"
     ]
    }
   ],
   "source": [
    "# Creamos una instancia de Matcher para buscar patrones en el texto utilizando el vocabulario cargado de spaCy\n",
    "matcher = Matcher(nlp.vocab) \n",
    "\n",
    "# Definimos un patrón de búsqueda: buscamos sustantivos propios (PROPN) con al menos uno o más ocurrencias\n",
    "# \"OP\" es un operador que indica la cantidad de veces que un patrón puede ocurrir.\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]  # 'OP': \"+\" significa que estamos buscando uno o más sustantivos propios consecutivos\n",
    "\n",
    "# Añadimos el patrón al matcher con un nombre único para identificarlo\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Imprimimos el número total de coincidencias encontradas\n",
    "print(len(matches))\n",
    "\n",
    "# Iteramos sobre las primeras 10 coincidencias para imprimir los detalles\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cd0c68a-8b5b-4dc8-b478-7608a404b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "(3232560085755078826, 12, 15) Sigismund Schlomo Freud\n",
      "(3232560085755078826, 415, 418) W. H. Auden\n",
      "(3232560085755078826, 0, 2) Sigmund Freud\n",
      "(3232560085755078826, 3, 5) /frɔɪd/ FROYD\n",
      "(3232560085755078826, 88, 90) Austrian Empire\n",
      "(3232560085755078826, 168, 170) United Kingdom\n",
      "(3232560085755078826, 275, 277) id\n",
      "(3232560085755078826, 17, 18) May\n",
      "(3232560085755078826, 21, 22) September\n",
      "(3232560085755078826, 72, 73) Freud\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]  # 'OP': \"+\" significa que estamos buscando uno o más sustantivos propios consecutivos\n",
    "\n",
    "\n",
    "# El parámetro 'greedy' especifica cómo debe comportarse el matcher cuando hay varias coincidencias posibles en el texto\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')  # 'greedy=\"LONGEST\"' indica que si hay coincidencias solapadas, se seleccionará la coincidencia más larga\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "\n",
    "print(len(matches))\n",
    "\n",
    "# Iteramos sobre las primeras 10 coincidencias para imprimir los detalles\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "12967a23-fd62-4a59-9a94-a2e91a46f5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "(3232560085755078826, 0, 2) Sigmund Freud\n",
      "(3232560085755078826, 3, 5) /frɔɪd/ FROYD\n",
      "(3232560085755078826, 12, 15) Sigismund Schlomo Freud\n",
      "(3232560085755078826, 17, 18) May\n",
      "(3232560085755078826, 21, 22) September\n",
      "(3232560085755078826, 72, 73) Freud\n",
      "(3232560085755078826, 84, 85) Freiberg\n",
      "(3232560085755078826, 88, 90) Austrian Empire\n",
      "(3232560085755078826, 102, 103) University\n",
      "(3232560085755078826, 104, 105) Vienna\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]  # 'OP': \"+\" significa que estamos buscando uno o más sustantivos propios consecutivos.\n",
    "# Este patrón busca secuencias de uno o más sustantivos propios (como nombres de personas, lugares o marcas).\n",
    "\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')  \n",
    "# 'greedy=\"LONGEST\"' significa que si hay coincidencias que se solapan, se seleccionará la coincidencia más larga.\n",
    "# Esto previene la selección de coincidencias más pequeñas que podrían estar contenidas en una coincidencia más grande.\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Ordenamos las coincidencias por el índice de inicio de cada coincidencia (x[1]) para organizar las coincidencias en el orden en que aparecen en el texto.\n",
    "matches.sort(key = lambda x: x[1])  # Ordena la lista de coincidencias en función del índice de inicio del token (x[1]).\n",
    "# Esto asegura que las coincidencias se procesen de acuerdo con su aparición en el texto, no en el orden en que fueron encontradas.\n",
    "\n",
    "# Imprimimos el número total de coincidencias encontradas en el texto.\n",
    "print(len(matches))\n",
    "\n",
    "for match in matches[:10]:  # Tomamos solo las primeras 10 coincidencias para mostrarlas.\n",
    "    print(match, doc[match[1]:match[2]])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e1a79554-bbae-45c3-9612-05be93c0d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(3232560085755078826, 128, 130) Freud lived\n",
      "(3232560085755078826, 154, 156) Freud left\n",
      "(3232560085755078826, 178, 180) Freud developed\n",
      "(3232560085755078826, 257, 259) Freud elaborated\n",
      "(3232560085755078826, 284, 286) Freud postulated\n",
      "(3232560085755078826, 331, 333) Freud developed\n",
      "(3232560085755078826, 423, 425) Freud describes\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Definimos un patrón de búsqueda: buscamos una secuencia de un sustantivo propio (PROPN) seguido de un verbo (VERB).\n",
    "# \"OP\" es un operador que indica la cantidad de veces que un patrón puede ocurrir.\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\"}]  \n",
    "# El primer elemento del patrón busca uno o más sustantivos propios consecutivos (como nombres de personas, lugares, etc.).\n",
    "# El segundo elemento busca un verbo. Esta combinación busca secuencias como \"John runs\" o \"Paris is growing\".\n",
    "\n",
    "# Añadimos el patrón al matcher con un nombre único para identificarlo\n",
    "# El parámetro 'greedy' especifica cómo debe comportarse el matcher cuando hay varias coincidencias posibles en el texto.\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')  \n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "matches.sort(key = lambda x: x[1])\n",
    "\n",
    "print(len(matches))  # Muestra cuántas coincidencias han sido encontradas en el texto.\n",
    "\n",
    "# Iteramos sobre las primeras 10 coincidencias para imprimir los detalles.\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "734b76dc-b67a-4c39-8c15-2380c09ee0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\"\n",
    "print (text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7090ca9d-4ee1-4a5c-898e-957169c0c9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = text.replace( \"`\", \"'\")\n",
    "print (text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b205546-fcae-4960-8d8a-53681feace58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coincidencias encontradas:  2\n",
      "(3232560085755078826, 47, 58) 'and what is the use of a book,'\n",
      "(3232560085755078826, 60, 67) 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "# Creamos una instancia de Matcher para buscar patrones en el texto utilizando el vocabulario cargado de spaCy\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# El patrón está compuesto por los siguientes elementos:\n",
    "pattern = [\n",
    "    {'ORTH': \"'\"},  # Busca una comilla simple ('), el parámetro 'ORTH' se usa para buscar tokens exactos por su forma ortográfica.\n",
    "    {'IS_ALPHA': True, \"OP\": \"+\"},  # Busca uno o más tokens alfabéticos (letras), 'IS_ALPHA': True indica que los tokens deben ser alfabéticos, 'OP': \"+\" indica que debe haber uno o más de estos tokens.\n",
    "    {'IS_PUNCT': True, \"OP\": \"*\"},  # Busca cero o más tokens de puntuación, 'IS_PUNCT' identifica tokens de puntuación, 'OP': \"*\" significa que puede haber ninguno o más de estos tokens.\n",
    "    {'ORTH': \"'\"}  # Busca otra comilla simple ('), similar al primer token, 'ORTH' asegura que sea la comilla exacta.\n",
    "]\n",
    "\n",
    "# Añadimos el patrón al matcher con un nombre único para identificarlo\n",
    "# El parámetro 'greedy' especifica cómo debe comportarse el matcher cuando hay varias coincidencias posibles en el texto\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')  # 'greedy=\"LONGEST\"' indica que si hay coincidencias solapadas, se seleccionará la coincidencia más larga\n",
    "\n",
    "# Procesamos el texto con el modelo 'nlp', creando un objeto doc que contiene todos los tokens procesados\n",
    "doc = nlp(text)\n",
    "\n",
    "# Ejecutamos el matcher sobre el 'doc' para encontrar todas las coincidencias del patrón definido\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Ordenamos las coincidencias por el índice de inicio de cada coincidencia (x[1]) para organizar las coincidencias en el orden que aparecen en el texto\n",
    "matches.sort(key = lambda x: x[1])\n",
    "\n",
    "# Imprimimos el número total de coincidencias encontradas\n",
    "print(\"Coincidencias encontradas: \", len(matches))  # Muestra cuántas coincidencias han sido encontradas en el texto\n",
    "\n",
    "# Iteramos sobre las primeras 10 coincidencias para imprimir los detalles\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3e3263bd-74f9-48f6-99c3-380ffa87af94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "# Encontrar al orador\n",
    "# Lista de lemas que estamos buscando, en este caso \"think\" y \"say\"\n",
    "speak_lemmas = [\"think\", \"say\"]\n",
    "\n",
    "# Creamos una instancia de Matcher para buscar patrones en el texto utilizando el vocabulario cargado de spaCy\n",
    "matcher = Matcher(nlp.vocab) \n",
    "\n",
    "# Definimos el patrón de búsqueda: buscamos secuencias que sigan la siguiente estructura\n",
    "pattern1 = [\n",
    "    {'ORTH': \"'\"},  # Busca una comilla simple ('), el parámetro 'ORTH' se usa para buscar tokens exactos por su forma ortográfica.\n",
    "    {'IS_ALPHA': True, \"OP\": \"+\"},  # Busca uno o más tokens alfabéticos (letras), 'IS_ALPHA': True indica que los tokens deben ser alfabéticos, 'OP': \"+\" significa que debe haber uno o más de estos tokens.\n",
    "    {'IS_PUNCT': True, \"OP\": \"*\"},  # Busca cero o más tokens de puntuación, 'IS_PUNCT' identifica tokens de puntuación, 'OP': \"*\" significa que puede haber ninguno o más de estos tokens.\n",
    "    {'ORTH': \"'\"},  # Busca otra comilla simple ('), similar al primer token, 'ORTH' asegura que sea la comilla exacta.\n",
    "    {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}},  # Busca un verbo en su forma de lema que esté en la lista 'speak_lemmas', es decir, busca \"think\" o \"say\".\n",
    "    {\"POS\": \"PROPN\", \"OP\": \"+\"},  # Busca uno o más sustantivos propios (PROPN), 'OP': \"+\" significa que debe haber al menos uno de estos sustantivos propios consecutivos.\n",
    "    {'ORTH': \"'\"},  # Busca una comilla simple ('), al igual que las anteriores.\n",
    "    {'IS_ALPHA': True, \"OP\": \"+\"},  # Busca uno o más tokens alfabéticos (letras).\n",
    "    {'IS_PUNCT': True, \"OP\": \"*\"},  # Busca cero o más tokens de puntuación.\n",
    "    {'ORTH': \"'\"}  # Busca una comilla simple ('), finalizando la secuencia.\n",
    "]\n",
    "\n",
    "# Añadimos el patrón al matcher con un nombre único para identificarlo\n",
    "# El parámetro 'greedy' especifica cómo debe comportarse el matcher cuando hay varias coincidencias posibles en el texto\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1], greedy='LONGEST')  # 'greedy=\"LONGEST\"' indica que si hay coincidencias solapadas, se seleccionará la coincidencia más larga\n",
    "\n",
    "\n",
    "doc = nlp(text) \n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Ordenamos las coincidencias por el índice de inicio de cada coincidencia (x[1]) para organizar las coincidencias en el orden que aparecen en el texto\n",
    "matches.sort(key = lambda x: x[1]) \n",
    "\n",
    "# Imprimimos el número total de coincidencias encontradas\n",
    "print(len(matches))\n",
    "\n",
    "# Iteramos sobre las primeras 10 coincidencias para imprimir los detalles\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754823e-385c-47db-9655-889b2eba3ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"alice.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Lista de lemas de verbos que buscamos: \"think\" y \"say\"\n",
    "speak_lemmas = [\"think\", \"say\"]\n",
    "\n",
    "# Reemplazamos las comillas invertidas por comillas simples en el texto de 'data'\n",
    "text = data[0][2][0].replace(\"`\", \"'\")  # 'data[0][2][0]' representa el texto que estamos procesando\n",
    "\n",
    "# Creamos una instancia de Matcher para buscar patrones en el texto utilizando el vocabulario cargado de spaCy\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Definimos tres patrones de búsqueda\n",
    "pattern1 = [\n",
    "    {'ORTH': \"'\"},  # Busca una comilla simple ('), el parámetro 'ORTH' se usa para buscar tokens exactos por su forma ortográfica.\n",
    "    {'IS_ALPHA': True, \"OP\": \"+\"},  # Busca uno o más tokens alfabéticos (letras).\n",
    "    {'IS_PUNCT': True, \"OP\": \"*\"},  # Busca cero o más tokens de puntuación.\n",
    "    {'ORTH': \"'\"},  # Busca otra comilla simple ('), cerrando la secuencia.\n",
    "    {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}},  # Busca un verbo en su forma de lema que esté en la lista 'speak_lemmas'.\n",
    "    {\"POS\": \"PROPN\", \"OP\": \"+\"},  # Busca uno o más sustantivos propios (PROPN).\n",
    "    {'ORTH': \"'\"},  # Busca una comilla simple ('), cerrando la secuencia.\n",
    "    {'IS_ALPHA': True, \"OP\": \"+\"},  # Busca uno o más tokens alfabéticos (letras).\n",
    "    {'IS_PUNCT': True, \"OP\": \"*\"},  # Busca cero o más tokens de puntuación.\n",
    "    {'ORTH': \"'\"}  # Busca una comilla simple ('), cerrando la secuencia.\n",
    "]\n",
    "\n",
    "pattern2 = [\n",
    "    {'ORTH': \"'\"},  # Comilla simple de apertura\n",
    "    {'IS_ALPHA': True, \"OP\": \"+\"},  # Tokens alfabéticos (letras)\n",
    "    {'IS_PUNCT': True, \"OP\": \"*\"},  # Tokens de puntuación\n",
    "    {'ORTH': \"'\"},  # Comilla simple de cierre\n",
    "    {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}},  # Verbo (de la lista 'speak_lemmas')\n",
    "    {\"POS\": \"PROPN\", \"OP\": \"+\"}  # Uno o más sustantivos propios\n",
    "]\n",
    "\n",
    "pattern3 = [\n",
    "    {\"POS\": \"PROPN\", \"OP\": \"+\"},  # Uno o más sustantivos propios (PROPN)\n",
    "    {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}},  # Verbo de la lista 'speak_lemmas'\n",
    "    {'ORTH': \"'\"},  # Comilla simple de apertura\n",
    "    {'IS_ALPHA': True, \"OP\": \"+\"},  # Tokens alfabéticos (letras)\n",
    "    {'IS_PUNCT': True, \"OP\": \"*\"},  # Tokens de puntuación\n",
    "    {'ORTH': \"'\"}  # Comilla simple de cierre\n",
    "]\n",
    "\n",
    "# Añadimos los tres patrones al matcher con un nombre único para identificarlos\n",
    "# El parámetro 'greedy' especifica cómo debe comportarse el matcher cuando hay varias coincidencias posibles en el texto\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')  # 'greedy=\"LONGEST\"' selecciona la coincidencia más larga en caso de solapamientos\n",
    "\n",
    "# Iteramos sobre el texto en 'data[0][2]' y procesamos cada uno con el matcher\n",
    "for text in data[0][2]:  # lista de textos a procesar\n",
    "    text = text.replace(\"`\", \"'\")  # Reemplazamos las comillas invertidas por comillas simples\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # Ordenamos las coincidencias por el índice de inicio de cada coincidencia para organizar las coincidencias en el orden que aparecen en el texto\n",
    "    matches.sort(key = lambda x: x[1])  # Ordena las coincidencias en función del índice de inicio del token\n",
    "\n",
    "    # Imprimimos el número total de coincidencias encontradas\n",
    "    print(print(\"Coincidencias encontradas: \", len(matches))  # Muestra cuántas coincidencias han sido encontradas en el texto\n",
    "\n",
    "    # Iteramos sobre las primeras 10 coincidencias para imprimir los detalles\n",
    "    for match in matches[:10]:\n",
    "        print(match, doc[match[1]:match[2]])  # Imprime el identificador del patrón y el fragmento de texto correspondiente al match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a137c775-4286-48aa-8fc7-af513c3c68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2 February', '2', 'February'), ('14 August', '4', 'August')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Expresión regular para encontrar fechas en formato 'd mes' o 'dd mes'\n",
    "pattern = r\"((\\d){1,2} (January|February|March|April|May|June|July|August|September|October|November|December))\"\n",
    "\n",
    "# Texto a analizar\n",
    "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
    "\n",
    "# Usamos re.findall() para encontrar todas las coincidencias con el patrón\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5451612f-a250-4ecd-ac79-b34037c71e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('14 August', '4', 'August')]\n"
     ]
    }
   ],
   "source": [
    "# La expresión regular está diseñada para encontrar fechas en el formato \"día mes\", pero en este caso, el texto tiene fechas en el formato \"mes día\".\n",
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "658f9d3a-eb53-4106-a0f3-d856ad2572b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('February 2', '', '', '', '', 'February 2', 'February ', 'February', '2'), ('14 August', '14 August', '4', ' August', 'August', '', '', '', '')]\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"(((\\d){1,2}( (January|February|March|April|May|June|July|August|September|October|November|December)))|(((January|February|March|April|May|June|July|August|September|October|November|December) )(\\d){1,2}))\"\n",
    "\n",
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1c36b84b-5887-4e06-a44e-cde8e7f64fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('February 2', '', '', 'February'), ('14 August', '14 August', 'August', '')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Expresión regular ajustada para capturar las fechas completas\n",
    "pattern = r\"((\\d{1,2} (January|February|March|April|May|June|July|August|September|October|November|December))|(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2})\"\n",
    "\n",
    "# Texto a analizar\n",
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "\n",
    "# Usamos re.findall() para encontrar todas las coincidencias\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "# Imprimimos las coincidencias\n",
    "print(matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "de0ffb16-bf86-4d71-9cbc-7f397bf3ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "February 2\n",
      "14 August\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Texto a analizar\n",
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "\n",
    "# Usamos re.finditer() para encontrar todas las coincidencias como un iterador\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "\n",
    "# Iteramos sobre el iterador y mostramos las coincidencias\n",
    "for match in iter_matches:\n",
    "    print(match.group())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dab288c5-4b33-4297-bd10-7b67b93e04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x000001D157E00FA0>\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "\n",
    "# Usamos re.finditer() para encontrar todas las coincidencias del patrón en el texto\n",
    "# re.finditer() devuelve un iterador de coincidencias\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "\n",
    "# Esto imprimirá la representación de la variable iter_matches (es un objeto generador)\n",
    "print(iter_matches)  # Aquí imprimimos el objeto iterador, pero no las coincidencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "98b8a144-8c9c-4363-9478-65e414244214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x000001D16949A1D0>\n",
      "<re.Match object; span=(15, 25), match='February 2'>\n",
      "<re.Match object; span=(49, 58), match='14 August'>\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "\n",
    "# Usamos re.finditer() para encontrar todas las coincidencias del patrón en el texto\n",
    "# re.finditer() devuelve un iterador de coincidencias\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "\n",
    "# Esto imprimirá la representación de la variable iter_matches (es un objeto generador)\n",
    "print(iter_matches)\n",
    "\n",
    "# Recorremos el iterador 'iter_matches' y mostramos cada coincidencia\n",
    "for hit in iter_matches: \n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9aff1c26-e23c-481c-a391-d5d8d6b0d091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "February 2\n",
      "14 August\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "\n",
    "# Usamos re.finditer() para encontrar todas las coincidencias del patrón en el texto\n",
    "# re.finditer() devuelve un iterador de coincidencias, es más eficiente que findall en términos de memoria\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "\n",
    "# Iteramos sobre cada coincidencia que devuelve el iterador 'iter_matches'\n",
    "for hit in iter_matches:\n",
    "    # Obtenemos la posición de inicio de la coincidencia con el método 'start()'\n",
    "    start = hit.start()\n",
    "    \n",
    "    # Obtenemos la posición de fin de la coincidencia con el método 'end()'\n",
    "    end = hit.end()\n",
    "    \n",
    "    # Usamos las posiciones de inicio y fin para extraer y mostrar la parte del texto que coincide\n",
    "    print(text[start:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "71ba8ef2-996b-431c-8312-cd59bbfa7ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Texto de ejemplo que contiene un número de teléfono\n",
    "text = \"This is a sample number 555-5555.\"\n",
    "\n",
    "# Crear un modelo vacío de spaCy en inglés\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Crear un Ruler (regla) y añadirlo al pipeline de spaCy\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")  # El 'entity_ruler' es utilizado para la coincidencia de patrones de entidades\n",
    "\n",
    "# Lista de entidades y patrones (fuente: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"SHAPE\": \"ddd\"},  # El patrón busca 3 dígitos\n",
    "                {\"ORTH\": \"-\", \"OP\": \"?\"},  # El guión es opcional\n",
    "                {\"SHAPE\": \"dddd\"}]}  # Luego se buscan 4 dígitos\n",
    "            ]\n",
    "\n",
    "# Añadir los patrones al 'entity_ruler'\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Procesar el texto con el modelo spaCy\n",
    "doc = nlp(text)  # El objeto 'doc' contiene el texto procesado y las entidades reconocidas\n",
    "\n",
    "# Extraer las entidades del 'doc' y mostrar su texto y la etiqueta de la entidad\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)  # Imprime el texto de la entidad y su etiqueta (como 'PHONE_NUMBER')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "037471fc-7123-4ef0-8d32-1e53d41d3587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('555-5555', '5', '5')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Definir el patrón de expresión regular para buscar un número de teléfono con el formato 555-5555\n",
    "pattern = r\"((\\d){3}-(\\d){4})\"\n",
    "\n",
    "text = \"This is a sample number 555-5555.\"\n",
    "\n",
    "# Buscar todas las coincidencias del patrón en el texto\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f42e8a85-e9c9-4fb5-80d8-42e9366447f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"This is a sample number (555) 555-5555.\"\n",
    "\n",
    "# Crear un modelo vacío de spaCy en inglés\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Crear el Ruler (regla) y añadirlo al pipeline de spaCy\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Lista de entidades y patrones (con expresión regular)\n",
    "patterns = [\n",
    "                {\n",
    "                    \"label\": \"PHONE_NUMBER\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"((\\\\d){3}-(\\\\d){4})\"}}]\n",
    "                }\n",
    "            ]\n",
    "\n",
    "# Añadir los patrones al 'entity_ruler'\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Procesar el texto con el modelo spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extraer las entidades del 'doc' y mostrar su texto y la etiqueta de la entidad\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)  # Imprime el texto de la entidad y su etiqueta (como 'PHONE_NUMBER')\n",
    "\n",
    "# La razón por la cual no hay resultado es que el EntityRuler de spaCy no puede hacer coincidencias de patrones que cruzan tokens, \n",
    "# lo que incluye situaciones en las que hay un guion (-) en el medio, como en los números de teléfono con formato 555-5555.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "09cb11fd-9f9b-486a-acea-46b361966962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5555555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "# Importar la librería necesaria para trabajar con spaCy\n",
    "import spacy\n",
    "\n",
    "# Texto de ejemplo que contiene un número de teléfono con 7 dígitos\n",
    "text = \"This is a sample number 5555555.\"\n",
    "\n",
    "# Crear un modelo vacío de spaCy en inglés\n",
    "nlp = spacy.blank(\"en\")  # 'en' es el código de idioma para inglés\n",
    "\n",
    "# Crear el Ruler (regla) y añadirlo al pipeline de spaCy\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")  # 'entity_ruler' es utilizado para la coincidencia de patrones de entidades\n",
    "\n",
    "# Lista de entidades y patrones (con expresión regular)\n",
    "patterns = [\n",
    "                {\n",
    "                    \"label\": \"PHONE_NUMBER\", \"pattern\": [{\"TEXT\": {\"REGEX\": r\"((\\d){5})\"}}]  # Usando raw string para evitar la advertencia\n",
    "                }\n",
    "            ]\n",
    "\n",
    "# Añadir los patrones al 'entity_ruler'\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Procesar el texto con el modelo spaCy\n",
    "doc = nlp(text)  # El objeto 'doc' contiene el texto procesado y las entidades reconocidas\n",
    "\n",
    "# Extraer las entidades del 'doc' y mostrar su texto y la etiqueta de la entidad\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)  # Imprime el texto de la entidad y su etiqueta (como 'PHONE_NUMBER')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2f59c634-66ad-40de-85c2-93b188f3f0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: 555-5555\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Crear un modelo vacío de spaCy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Crear un matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Definir el patrón para un número de teléfono con formato \"xxx-xxxx\"\n",
    "pattern = [\n",
    "    {\"ORTH\": {\"REGEX\": r\"\\d{3}\"}},  # tres dígitos\n",
    "    {\"ORTH\": \"-\"},                   # guion\n",
    "    {\"ORTH\": {\"REGEX\": r\"\\d{4}\"}}    # cuatro dígitos\n",
    "]\n",
    "\n",
    "# Agregar el patrón al matcher\n",
    "matcher.add(\"PHONE_NUMBER\", [pattern])\n",
    "\n",
    "# Texto de ejemplo\n",
    "text = \"This is a sample number 555-5555.\"\n",
    "\n",
    "# Procesar el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Buscar las coincidencias\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Mostrar las coincidencias encontradas\n",
    "for match_id, start, end in matches:\n",
    "    print(f\"Match found: {doc[start:end].text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "18dabeb2-6e02-46d0-bc63-407e6f44784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Texto de ejemplo que contiene varios nombres con \"Paul\"\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "\n",
    "# Definir el patrón de búsqueda\n",
    "pattern = r\"Paul [A-Z]\\w+\"  # Busca \"Paul\" seguido de un apellido que comienza con mayúscula\n",
    "\n",
    "# Buscar todas las coincidencias en el texto\n",
    "matches = re.finditer(pattern, text)\n",
    "\n",
    "# Iterar sobre los resultados y mostrarlos\n",
    "for match in matches:\n",
    "    print(match)  # Imprime el objeto de coincidencia, que contiene información sobre cada hallazgo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5f303415-a994-43aa-9a97-2a8fd8bb639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Span  # Para crear nuevas entidades dentro del documento de spaCy\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "\n",
    "# Expresión regular para encontrar nombres que comiencen con \"Paul\" seguido de un apellido\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Guardar las entidades detectadas originalmente por spaCy (aunque en este caso no hay ninguna)\n",
    "original_ents = list(doc.ents)\n",
    "\n",
    "# Lista para almacenar nuevas entidades detectadas con la expresión regular\n",
    "mwt_ents = []\n",
    "\n",
    "# Usar la expresión regular para encontrar nombres en el texto\n",
    "for match in re.finditer(pattern, doc.text):  \n",
    "    start, end = match.span()  # Obtener las posiciones inicial y final del match en el texto\n",
    "    span = doc.char_span(start, end)  # Convertir la posición en un objeto de spaCy (span)\n",
    "    \n",
    "    # Si spaCy logra mapear correctamente la posición en tokens, se guarda la entidad encontrada\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))  \n",
    "\n",
    "# Convertir los resultados de la expresión regular en entidades de spaCy\n",
    "for ent in mwt_ents:\n",
    "    start, end, name = ent  # Extraer datos de la entidad detectada\n",
    "    per_ent = Span(doc, start, end, label=\"PERSON\")  # Crear una entidad con etiqueta \"PERSON\"\n",
    "    original_ents.append(per_ent)  # Agregar la nueva entidad a la lista original\n",
    "\n",
    "# Asignar las entidades modificadas al documento de spaCy\n",
    "doc.ents = original_ents\n",
    "\n",
    "# Imprimir todas las entidades reconocidas en el texto\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bd30c3a7-4eb7-44cf-87a3-99611a390bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "American NORP\n",
      "Paul Hollywood PERSON\n",
      "British NORP\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host.\"\n",
    "pattern = r\"Hollywood\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "08837c62-4def-414a-ac1b-bcdd54a0c8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(44, 53), match='Hollywood'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 9 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     original_ents\u001b[38;5;241m.\u001b[39mappend(per_ent)  \u001b[38;5;66;03m# Agregar la nueva entidad a la lista original de entidades\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Asignar las entidades modificadas al documento de spaCy\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;241m=\u001b[39m original_ents\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:798\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:835\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 9 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "# Lista para almacenar las nuevas entidades detectadas por la expresión regular\n",
    "mwt_ents = []\n",
    "\n",
    "# Obtener las entidades originales del documento procesado con spaCy\n",
    "original_ents = list(doc.ents)\n",
    "\n",
    "# Iterar sobre todas las coincidencias encontradas con la expresión regular en el texto\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    print(match)  # Imprimir el objeto de la coincidencia para depuración\n",
    "    \n",
    "    start, end = match.span()  # Obtener las posiciones de inicio y fin del match en el texto\n",
    "    span = doc.char_span(start, end)  # Convertir la posición en un objeto Span de spaCy\n",
    "    \n",
    "    # Si spaCy logra mapear correctamente la posición en tokens, se guarda la entidad encontrada\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "# Convertir los resultados de la expresión regular en entidades de spaCy con la etiqueta \"CINEMA\"\n",
    "for ent in mwt_ents:\n",
    "    start, end, name = ent  # Extraer datos de la entidad detectada\n",
    "    per_ent = Span(doc, start, end, label=\"CINEMA\")  # Crear una nueva entidad con etiqueta \"CINEMA\"\n",
    "    original_ents.append(per_ent)  # Agregar la nueva entidad a la lista original de entidades\n",
    "\n",
    "# Asignar las entidades modificadas al documento de spaCy\n",
    "doc.ents = original_ents\n",
    "\n",
    "\n",
    "# Cuando agregas entidades manualmente con doc.ents, si una palabra ya pertenece a una entidad existente y se intenta asignarla a otra entidad, se genera un error.\n",
    "# En este caso, parece que una parte de la entidad detectada con re.finditer() ya estaba en una entidad previa de spaCy, provocando el error.\n",
    "# La función filter_spans() de spaCy filtra las entidades superpuestas, dejando solo la más larga o prioritaria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2318b460-fc76-4da9-b471-ec7ea70b4be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "American NORP\n",
      "Paul Hollywood PERSON\n",
      "British NORP\n"
     ]
    }
   ],
   "source": [
    "# Importamos la función filter_spans de spaCy, que ayuda a eliminar superposiciones en entidades\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "# Aplicamos la función filter_spans para eliminar entidades solapadas (superpuestas)\n",
    "filtered = filter_spans(original_ents)\n",
    "\n",
    "# Asignamos las entidades filtradas al documento\n",
    "doc.ents = filtered\n",
    "\n",
    "# Recorremos e imprimimos las entidades finales del documento\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cb6ae71e-5d50-439a-a2e2-1b45e80663fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar el modelo pequeño de spaCy en inglés\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Procesar el texto con el modelo de spaCy\n",
    "doc = nlp(\"Britain is a place. Mary is a doctor.\")\n",
    "\n",
    "# Recorrer las entidades e imprimir su texto y etiqueta\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ba1a9b05-935b-4658-88e8-b6d50366bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_gpe(doc)>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos la clase Language de spaCy para definir un componente personalizado\n",
    "from spacy.language import Language\n",
    "\n",
    "# Definimos un nuevo componente para eliminar entidades del tipo \"GPE\" (países, ciudades, ubicaciones)\n",
    "@Language.component(\"remove_gpe\")\n",
    "def remove_gpe(doc):\n",
    "    # Copiamos las entidades originales en una lista mutable\n",
    "    original_ents = list(doc.ents)\n",
    "    \n",
    "    # Iteramos sobre las entidades del documento\n",
    "    for ent in doc.ents:\n",
    "        # Si la entidad es de tipo \"GPE\", la eliminamos de la lista\n",
    "        if ent.label_ == \"GPE\":\n",
    "            original_ents.remove(ent)\n",
    "    \n",
    "    # Asignamos la lista modificada de entidades al documento\n",
    "    doc.ents = original_ents\n",
    "    \n",
    "    # Devolvemos el documento modificado\n",
    "    return doc\n",
    "\n",
    "# Agregamos el nuevo componente al pipeline de spaCy\n",
    "nlp.add_pipe(\"remove_gpe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "35e143e8-4aff-4121-812d-717719a3d997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'remove_gpe': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'remove_gpe': []},\n",
       " 'attrs': {'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a54cd90b-0af7-4217-89dd-f059948fead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "# Procesar el texto con el modelo de spaCy\n",
    "doc = nlp(\"Britain is a place. Mary is a doctor.\")\n",
    "\n",
    "# Recorrer las entidades e imprimir su texto y etiqueta\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d24be8-cf29-4824-bff5-6928023e3113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
